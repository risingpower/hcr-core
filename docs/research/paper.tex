\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{authblk}
\usepackage{listings}
\usepackage{caption}

% Hyperref config
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue
}

% Listing style for code blocks
\lstset{
    basicstyle=\small\ttfamily,
    frame=single,
    breaklines=true,
    columns=fullflexible
}

% Title
\title{Per-Level Routing Error Compounds Exponentially:\\A Negative Result on Embedding-Based Hierarchical Retrieval at Scale}

\author{Jason Crispin}
\affil{Independent Researcher}
\date{February 2026}

\begin{document}

\maketitle

% ============================================================
% ABSTRACT
% ============================================================
\begin{abstract}
Hierarchical retrieval promises token-efficient context for large language model (LLM) systems by routing queries through a tree of summaries rather than searching a flat index. We present HCR (Hierarchical Context Retrieval), a coarse-to-fine retrieval system that uses embedding-based routing through LLM-generated contrastive summaries organized in a tree built by bisecting k-means clustering. We introduce \textbf{per-level routing epsilon}, a novel metric that measures routing error at each tree level independently, enabling diagnosis of where hierarchical retrieval fails. We tested 12 configurations across two corpus scales (315 and 21,897 chunks), varying beam width (3--8), embedding model (MiniLM 384-dim, mpnet 768-dim), routing strategy (cosine, cross-encoder, BM25 hybrid), and summary representation. At small scale, the best configuration achieved nDCG@10\,=\,0.580 versus a flat dense retrieval + cross-encoder baseline of 0.835. At medium scale, HCR collapsed to nDCG@10\,=\,0.094 versus 0.749 for the baseline---a catastrophic failure triggered by exponential error compounding across tree depth. Per-level epsilon analysis confirms the mechanism: routing error at each level compounds as $\prod(1-\varepsilon_l)$, and at depth~5, even moderate per-level error produces near-random leaf selection. Beam width sweep confirms that the tree structure is sound (correct branches exist) but embedding-based cosine routing cannot reliably select them. We document five empirical patterns---DPI summary blindness, cross-encoder routing damage, beam width as diagnostic, embedding saturation, and BM25 routing sparsity---that inform future hierarchical retrieval design. We argue that per-level epsilon should be a standard evaluation metric for any hierarchical retrieval system.
\end{abstract}

% ============================================================
% 1. INTRODUCTION
% ============================================================
\section{Introduction}

The growth of LLM context windows has not eliminated the need for precise retrieval. Larger windows enable longer inputs but do not guarantee that the model attends to the right information. For agentic systems operating under token budgets---where every token of context costs inference time and money---retrieval quality matters more than retrieval quantity. The goal is \emph{minimum viable context}: the fewest tokens that produce a correct answer.

Hierarchical retrieval offers an appealing path to this goal. By organizing documents into a tree of increasingly specific summaries, a system can route queries coarse-to-fine, pruning irrelevant branches early and delivering only the most relevant leaf content. Systems like RAPTOR~\cite{raptor} and LATTICE~\cite{lattice} have demonstrated that tree-structured indices improve performance on complex reasoning tasks. The theoretical basis is straightforward: a tree with branching factor $b$ and depth $d$ reduces the search space from $N$ to $O(b \cdot d)$ scoring operations, with proportional token savings.

But this efficiency depends on routing accuracy. The Data Processing Inequality guarantees that information is lost ascending the tree---summaries are lossy compressions of their children~\cite{cover2006}. If the routing mechanism makes errors, those errors compound multiplicatively across levels. At depth $d$ with per-level error rate $\varepsilon$, end-to-end recall degrades as $(1-\varepsilon)^d$. This compounding is structural: it applies to any system that makes sequential, irrevocable routing decisions through a hierarchy.

We built HCR to test whether embedding-based routing through LLM-generated summaries could achieve sufficient accuracy to make hierarchical retrieval practical under token constraints. We tested 12 configurations systematically, varying every major design parameter. The result is a clear negative: \textbf{embedding-based tree routing fails at scale due to exponential error compounding.} At 315 chunks, the best configuration reached nDCG@10\,=\,0.580. At 21,897 chunks (a 70$\times$ increase), performance collapsed to 0.094---worse than random retrieval with the token budget.

This paper makes three contributions:

\begin{enumerate}
    \item \textbf{Per-level routing epsilon ($\varepsilon$):} A novel metric that measures routing accuracy at each tree level independently. No prior system has exposed or measured per-level routing decisions. This metric reveals \emph{where} hierarchical retrieval fails, not just \emph{that} it fails.

    \item \textbf{A systematic negative result:} Twelve configurations tested across two scales, two embedding models, four routing strategies, and three summary representations. The failure is not due to a single bad design choice---it is structural to embedding-based routing on tree-organized summaries.

    \item \textbf{Five empirical patterns} that explain specific failure mechanisms and inform the design of future hierarchical retrieval systems.
\end{enumerate}

% ============================================================
% 2. RELATED WORK
% ============================================================
\section{Related Work}

\subsection{Hierarchical Retrieval Systems}

RAPTOR~\cite{raptor} introduced tree-structured retrieval for LLMs, using bottom-up clustering with LLM-generated summaries at each level. Critically, RAPTOR's own experiments show that its \emph{collapsed tree} mode (flat retrieval over all nodes at all levels) outperforms strict top-down tree traversal. This finding is theoretically predicted by error compounding analysis~\cite{cover2006} and is a key motivation for our investigation.

LATTICE~\cite{lattice} (UT Austin, October 2025) is the closest system to HCR. It uses LLM-guided tree navigation with logarithmic complexity, sharing the philosophy of hierarchical routing through summaries. LATTICE differs in using LLM-as-judge scoring at each level (expensive but potentially more accurate) and sequential best-first traversal rather than parallel beam search. Neither RAPTOR nor LATTICE report per-level routing accuracy.

HIRO~\cite{hiro} implements DFS-style querying on RAPTOR trees with delta-threshold pruning, the closest prior work to elimination-based traversal. GraphRAG~\cite{graphrag} uses hierarchical community detection in a graph structure rather than a tree, at substantially higher cost.

\subsection{Flat Retrieval Baselines}

Modern flat retrieval combines dense embedding search with cross-encoder reranking~\cite{nogueira2019}. Cross-encoders (e.g., MS-MARCO trained models) jointly encode query-document pairs, achieving substantially higher accuracy than bi-encoder similarity at the cost of $O(N)$ inference. For corpora under ${\sim}100$K chunks, flat + cross-encoder retrieval is practical and provides a strong baseline.

\subsection{Theoretical Foundations}

The fragility of hierarchical elimination is well-established in information theory. The Data Processing Inequality~\cite{cover2006} guarantees that mutual information between a query and a node's content cannot increase as one ascends the tree. Branch-and-bound optimality requires admissible bounds on descendant relevance---a condition that embedding similarity to summary text does not satisfy. The cluster hypothesis~\cite{vanrijsbergen1979} (that relevant documents cluster together) is necessary for tree-based retrieval but fails in 46\% of broad collections~\cite{voorhees1985}.

Three independent analyses converge on the same formula: per-level miss rate $\varepsilon$ compounds as $(1-\varepsilon)^d$, making deep trees with imperfect routing exponentially fragile. Beam search mitigates this by maintaining $k$ candidates, transforming recall to approximately $(1-(1-p)^k)^d$, but cannot eliminate the fundamental compounding.

\subsection{The Measurement Gap}

No prior system has measured per-level routing accuracy. RAPTOR, LATTICE, HIRO, and other hierarchical systems report end-to-end metrics (nDCG, recall, F1) but do not expose the internal routing decisions that determine whether the correct branches are traversed at each level. This makes it impossible to diagnose \emph{where} hierarchical retrieval fails. Our per-level epsilon metric fills this gap.

% ============================================================
% 3. METHOD
% ============================================================
\section{Method}

\subsection{Tree Construction}

HCR builds a retrieval tree through a two-stage process: hierarchical clustering followed by LLM-based summary generation.

\paragraph{Clustering.} We use top-down bisecting k-means in embedding space. Starting from the full corpus, each level splits into $b$ clusters (target branching factor), recursing until maximum depth $d$ is reached or clusters are small enough to serve as leaves. For the small corpus (315 chunks), the tree has depth~3 with branching factor~8, producing a structure: L0: 1~root, L1: 8~nodes, L2: 64~nodes, L3--L4: 333~leaves. For the medium corpus (21,897 chunks), depth increases to~5 with 25,716 total nodes (3,819 internal).

\paragraph{Summary generation.} For each internal node, an LLM generates a structured routing summary containing:
\begin{itemize}
    \item \textbf{theme}: cluster topic (1--2 sentences)
    \item \textbf{includes}: 5--8 specific topics covered
    \item \textbf{excludes}: 3--5 topics NOT covered (contrastive, informed by sibling nodes)
    \item \textbf{key\_entities}: 5--10 proper nouns and product names
    \item \textbf{key\_terms}: 8--15 searchable keywords, abbreviations, synonyms
\end{itemize}
The contrastive design (explicitly stating what a node does \emph{not} cover, based on its siblings) is intended to improve discriminability between sibling nodes. Summaries are concatenated into a single text string for embedding.

\paragraph{Embedding.} Summary text is embedded using sentence-transformers models. We tested two models: all-MiniLM-L6-v2 (384 dimensions) and all-mpnet-base-v2 (768 dimensions). All embeddings are L2-normalized. When changing embedding models, the tree must be rebuilt---using a different model's embeddings with a tree clustered in a different embedding space degrades performance (Section~\ref{sec:results-small}).

\subsection{Query-Time Routing}

At query time, HCR performs beam search traversal through the tree.

\paragraph{Scoring cascade.} At each level, all children of the current frontier nodes are scored against the query. For internal nodes, scoring uses cosine similarity between the query embedding and the node's summary embedding. For leaf nodes, a cross-encoder (cross-encoder/ms-marco-MiniLM-L-6-v2) reranks the top candidates by jointly encoding query-chunk pairs.

Cross-encoder scoring is \emph{disabled} for internal nodes. We empirically determined that the MS-MARCO cross-encoder, trained on natural language query-passage pairs, performs poorly on structured routing summary text (Section~\ref{sec:patterns}, Pattern~2).

\paragraph{Beam selection.} At each level, the top $k$ candidates (beam width) are retained. Path-relevance EMA smooths scores across depth: $s_{\mathrm{new}} = \alpha \cdot s_{\mathrm{current}} + (1-\alpha) \cdot s_{\mathrm{parent}}$, with $\alpha = 0.5$. An MMR-style diversity penalty discourages multiple beams from exploring the same branch ($\lambda_{\mathrm{diversity}} = 0.3$).

\paragraph{Token budget.} Retrieved leaf chunks are packed greedily up to a 400-token budget. We use relevance-minus-redundancy scoring for chunk selection within the budget.

\subsection{Per-Level Routing Epsilon ($\varepsilon$)---Novel Metric}
\label{sec:epsilon}

We introduce per-level routing epsilon to measure routing accuracy at each tree level independently.

\paragraph{Definition.} For a set of evaluation queries $Q$ with known relevant chunks, and a tree with levels $l = 0, 1, \ldots, L$:
\begin{equation}
\varepsilon_l = 1 - \frac{|\{q \in Q : \mathrm{ancestor}_l(\mathrm{gold}(q)) \in \mathrm{beam}_l(q)\}|}{|Q|}
\end{equation}
where $\mathrm{ancestor}_l(\mathrm{gold}(q))$ is the ancestor at level $l$ of any gold-relevant chunk for query $q$, and $\mathrm{beam}_l(q)$ is the set of nodes in the beam at level $l$ for query $q$. A query is ``correct'' at level $l$ if \emph{any} of its gold chunks' ancestors appear in the beam.

\paragraph{Interpretation.}
$\varepsilon_l = 0$: perfect routing at level $l$ (all relevant branches in beam).
$\varepsilon_l = 1$: random routing (no relevant branches in beam).

\paragraph{Theoretical prediction.} If routing errors are independent across levels, end-to-end recall is approximately:
\begin{equation}
R \approx \prod_{l=1}^{L} (1 - \varepsilon_l)
\end{equation}
At depth~5 with $\varepsilon = 0.15$ per level: $R \approx 0.85^5 = 0.444$. At $\varepsilon = 0.30$: $R \approx 0.70^5 = 0.168$.

\paragraph{Why this hasn't been measured.} Prior hierarchical retrieval systems (RAPTOR, LATTICE, HIRO) report only end-to-end metrics. They do not instrument or expose the beam contents at each intermediate level. Per-level epsilon requires (a)~ground-truth relevant chunks with known positions in the tree, and (b)~logging of beam state at every tree level during evaluation. HCR instruments both.

% ============================================================
% 4. EXPERIMENTAL SETUP
% ============================================================
\section{Experimental Setup}

\subsection{Corpus}

We evaluate at two scales using the GitLab public handbook as source material:

\begin{table}[h]
\centering
\caption{Corpus statistics}
\label{tab:corpus}
\begin{tabular}{lrr}
\toprule
 & \textbf{Small} & \textbf{Medium} \\
\midrule
Documents & 50 & 3,885 \\
Chunks & 315 & 21,897 \\
Total tokens & 125,435 & 8,804,259 \\
Mean chunk tokens & 398 & 402 \\
Source & GitLab handbook (sample) & GitLab handbook (full) \\
\bottomrule
\end{tabular}
\end{table}

The GitLab handbook is a large, publicly available organizational knowledge base covering company processes, policies, engineering practices, and operational procedures. It provides realistic heterogeneity (technical documentation, HR policies, financial processes, engineering guides) representative of organizational retrieval tasks.

\subsection{Queries}

\textbf{Small:} 50 queries stratified across 9 categories: single\_branch (12), entity\_spanning (10), dpi (7), multi\_hop (5), comparative (5), aggregation (2), temporal (2), ambiguous (2), ood (5).

\textbf{Medium:} 150 queries with proportional stratification across the same categories.

Queries were generated by an LLM from source chunks, with category labels reflecting the structural retrieval challenge each query poses (e.g., ``dpi'' queries target specific details that summaries may lose; ``comparative'' queries require information from multiple branches).

\subsection{Baselines}

Three baselines of increasing sophistication:
\begin{enumerate}
    \item \textbf{BM25:} Okapi BM25 with $k_1 = 1.5$, $b = 0.75$. Lexical baseline.
    \item \textbf{Hybrid-RRF:} BM25 + dense retrieval combined via reciprocal rank fusion. Standard hybrid approach.
    \item \textbf{Flat+CE:} Dense retrieval followed by cross-encoder reranking (cross-encoder/ms-marco-MiniLM-L-6-v2). This is the \textbf{kill baseline}---HCR must exceed this to validate the hierarchical approach.
\end{enumerate}

\subsection{HCR Configurations}

We tested 12 configurations (v1--v12), of which 11 are valid (v1 was discarded due to an implementation bug in the tree builder). Configurations are organized by the experimental question they address (Table~\ref{tab:configs}).

\begin{table}[h]
\centering
\caption{HCR configurations tested}
\label{tab:configs}
\small
\begin{tabular}{llclll}
\toprule
\textbf{Config} & \textbf{Variable} & \textbf{Beam} & \textbf{Embed} & \textbf{Routing} & \textbf{Key Change} \\
\midrule
v2 & Baseline & 3 & MiniLM & CE all & First proper tree \\
v3 & Routing & 3 & MiniLM & Cosine & Removed CE from routing \\
v4 & Beam width & 5 & MiniLM & Cosine & Wider beam \\
v5 & Beam ceiling & 8 & MiniLM & Cosine & Maximum beam \\
v6 & Summary text & 5 & MiniLM & Cosine & Enriched embed text \\
v7 & Summary text & 5 & MiniLM & Cosine & Contrastive prompts \\
v8 & Summary text & 5 & MiniLM & Cosine & Content snippets \\
v9 & Embed model & 5 & mpnet & Cosine & No tree rebuild \\
v10 & Embed + rebuild & 5 & mpnet & Cosine & Rebuilt tree \\
v11 & Ceiling & 8 & mpnet & Cosine & Best config, max beam \\
v12 & Hybrid routing & 5 & mpnet & BM25+cos & BM25 hybrid \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Metrics}

\textbf{Standard IR:} nDCG@10 (normalized discounted cumulative gain), Recall@10 (fraction of gold chunks in top~10), MRR (mean reciprocal rank).

\textbf{Token efficiency:} MeanTok (average tokens used under 400-token budget).

\textbf{Novel:} Per-level $\varepsilon$ (routing error at each tree level, Section~\ref{sec:epsilon}); Sibling Distinctiveness (SD, mean pairwise cosine distance between sibling embeddings; values above 0.15 indicate sufficient separation).

\subsection{Fail-Fast Protocol}

We implement a fail-fast evaluation sequence with kill criteria at each step:
\begin{enumerate}
    \item \textbf{Topology check:} SD $< 0.15 \Rightarrow$ KILL
    \item \textbf{Routing check:} L1 $\varepsilon > 0.05$ on easy queries $\Rightarrow$ WARNING
    \item \textbf{nDCG check:} HCR nDCG $<$ Flat+CE nDCG $- 0.15 \Rightarrow$ KILL
    \item \textbf{Token efficiency curve:} diminishing returns check
    \item \textbf{Full evaluation:} all metrics
\end{enumerate}

% ============================================================
% 5. RESULTS
% ============================================================
\section{Results}

\subsection{Small-Scale Results (315 chunks)}
\label{sec:results-small}

\paragraph{Baselines.} Table~\ref{tab:baselines-small} shows baseline performance at small scale.

\begin{table}[h]
\centering
\caption{Baseline results, small corpus (315 chunks, 50 queries)}
\label{tab:baselines-small}
\begin{tabular}{lcccc}
\toprule
\textbf{System} & \textbf{nDCG@10} & \textbf{Recall@10} & \textbf{MRR} & \textbf{MeanTok} \\
\midrule
BM25 & 0.705 & 0.82 & 0.669 & 333 \\
Hybrid-RRF & 0.719 & 0.90 & 0.662 & 343 \\
Flat+CE & \textbf{0.835} & \textbf{0.94} & \textbf{0.803} & 354 \\
\bottomrule
\end{tabular}
\end{table}

Flat+CE substantially outperforms both lexical and hybrid baselines, establishing a strong kill criterion of nDCG@10\,=\,0.835.

\paragraph{HCR progression.} Table~\ref{tab:hcr-small} shows the systematic improvement across configurations.

\begin{table}[h]
\centering
\caption{HCR results, small corpus (315 chunks, 50 queries)}
\label{tab:hcr-small}
\small
\begin{tabular}{llccccccc}
\toprule
\textbf{Config} & \textbf{Key Variable} & \textbf{nDCG} & \textbf{R@10} & \textbf{MRR} & \textbf{Tok} & \textbf{L1\,$\varepsilon$} & \textbf{L2\,$\varepsilon$} \\
\midrule
v2 & CE all, beam=3 & 0.318 & 0.40 & 0.298 & 234 & 0.32 & 0.62 \\
v3 & Cosine, beam=3 & 0.287 & 0.32 & 0.281 & 235 & 0.32 & 0.58 \\
v4 & Beam=5 & 0.420 & 0.46 & 0.411 & 255 & 0.24 & 0.46 \\
v5 & Beam=8 (MiniLM) & 0.509 & 0.58 & 0.490 & 297 & 0.14 & 0.36 \\
v6 & Enriched text & 0.493 & 0.54 & --- & 249 & 0.16 & 0.36 \\
v7 & Contrastive prompts & 0.484 & --- & --- & 269 & 0.14 & 0.44 \\
v8 & Content snippets & 0.485 & --- & --- & 279 & 0.16 & 0.42 \\
v9 & mpnet, no rebuild & 0.450 & 0.54 & --- & 296 & 0.18 & 0.38 \\
v10 & mpnet, rebuilt & 0.540 & 0.62 & 0.516 & 231 & 0.24 & 0.42 \\
v11 & mpnet, beam=8 & \textbf{0.580} & \textbf{0.66} & \textbf{0.556} & 290 & \textbf{0.06} & \textbf{0.28} \\
v12 & BM25 hybrid & 0.465 & --- & --- & --- & --- & --- \\
\bottomrule
\end{tabular}
\end{table}

Key observations at small scale:

\begin{enumerate}
    \item \textbf{Beam width is the strongest lever.} Increasing beam from 3 to 8 (v3$\to$v5) improved nDCG by +0.222 and halved L1\,$\varepsilon$ from 0.32 to 0.14. This is monotonic across all beam widths tested.

    \item \textbf{Cross-encoder routing is net negative.} v2 (CE at all levels) vs v3 (cosine-only) shows CE slightly \emph{improves} nDCG (+0.031) but with worse recall (0.40 vs 0.32). Analysis revealed the MS-MARCO cross-encoder flipped 26 correct cosine decisions to incorrect while saving only 14---a net loss of 12 correct routing decisions.

    \item \textbf{Summary text enrichment helps, then plateaus.} v6 (enriched) improved L1\,$\varepsilon$ from 0.24 to 0.16, but contrastive prompts (v7) and content snippets (v8) provided no additional benefit.

    \item \textbf{Embedding model matters, but only with tree rebuild.} mpnet without rebuilding the tree (v9) \emph{decreased} nDCG by 8.7\% compared to MiniLM (v6). After rebuilding (v10), nDCG improved to 0.540---the best at beam=5.

    \item \textbf{Best configuration (v11):} mpnet with rebuilt tree at beam=8 achieves nDCG\,=\,0.580, L1\,$\varepsilon$\,=\,0.06, L2\,$\varepsilon$\,=\,0.28. The gap to Flat+CE remains 0.255.
\end{enumerate}

\paragraph{Per-level epsilon analysis.} Table~\ref{tab:epsilon} shows the full per-level epsilon for the default HCR configuration.

\begin{table}[h]
\centering
\caption{Per-level epsilon, HCR default configuration (small corpus)}
\label{tab:epsilon}
\begin{tabular}{lccc}
\toprule
\textbf{Level} & \textbf{Queries Evaluated} & \textbf{Correct in Beam} & $\boldsymbol{\varepsilon}$ \\
\midrule
L0 (root) & 50 & 50 & 0.00 \\
L1 & 50 & 39 & 0.22 \\
L2 & 50 & 25 & 0.50 \\
L3 & 50 & 21 & 0.58 \\
L4 (leaves) & 31 & 5 & 0.84 \\
\bottomrule
\end{tabular}
\end{table}

The monotonic degradation of routing accuracy with depth is stark. At L4, only 5 of 31 queries that reached the leaf level had correct chunks in the beam---effectively random selection. The bottleneck shifts with beam width: at beam=8 (v11), L1\,$\varepsilon$ drops to 0.06, but L2 remains at 0.28, indicating that deeper levels become the binding constraint as shallow routing improves.

\subsection{Medium-Scale Results (21,897 chunks)}

At medium scale, the tree grows to 25,716 nodes (3,819 internal) with depth~5 and 8 root branches. Sibling distinctiveness is 0.445 (passing the 0.15 threshold).

\begin{table}[h]
\centering
\caption{Baseline results, medium corpus (21,897 chunks, 150 queries)}
\label{tab:baselines-medium}
\begin{tabular}{lcccc}
\toprule
\textbf{System} & \textbf{nDCG@10} & \textbf{Recall@10} & \textbf{MRR} & \textbf{MeanTok} \\
\midrule
BM25 & 0.522 & 0.69 & 0.472 & 336 \\
Hybrid-RRF & 0.594 & 0.77 & 0.548 & 352 \\
Flat+CE & \textbf{0.749} & \textbf{0.86} & \textbf{0.717} & 349 \\
\bottomrule
\end{tabular}
\end{table}

All baselines degrade from small to medium scale, as expected with a 70$\times$ increase in corpus size. Flat+CE drops from 0.835 to 0.749 but remains the strongest baseline.

\paragraph{HCR result.} The fail-fast protocol killed HCR at step~3 (nDCG check):

\begin{table}[h]
\centering
\caption{HCR fail-fast result, medium corpus}
\label{tab:failfast}
\begin{tabular}{lr}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Sibling Distinctiveness & 0.445 (pass) \\
HCR nDCG@10 & \textbf{0.094} \\
Flat+CE nDCG@10 & 0.749 \\
Gap (HCR -- Flat+CE) & \textbf{$-$0.655} \\
Kill threshold & $-$0.15 \\
Outcome & \textbf{KILLED} \\
\bottomrule
\end{tabular}
\end{table}

HCR nDCG collapsed from 0.580 (small, best config) to 0.094 (medium)---an 84\% relative decline. The gap of $-0.655$ to the kill baseline is more than four times the kill threshold.

\subsection{Scale-Dependent Collapse}

Table~\ref{tab:scale} directly compares small and medium performance.

\begin{table}[h]
\centering
\caption{Scale comparison}
\label{tab:scale}
\begin{tabular}{lccr}
\toprule
\textbf{Metric} & \textbf{Small (315)} & \textbf{Medium (21,897)} & \textbf{Change} \\
\midrule
Flat+CE nDCG & 0.835 & 0.749 & $-$10.3\% \\
HCR nDCG (best) & 0.580 & 0.094 & $\boldsymbol{-83.8\%}$ \\
HCR MeanTok & 290 & 292 & +0.7\% \\
Tree depth & 3 & 5 & +2 levels \\
Internal nodes & ${\sim}$73 & 3,819 & +52$\times$ \\
\bottomrule
\end{tabular}
\end{table}

The key observation: \textbf{token efficiency is maintained} (290$\to$292 tokens) while \textbf{accuracy collapses}. The tree successfully limits token usage, but the tokens it selects are wrong. The additional two levels of depth at medium scale (depth~3$\to$5) multiply routing errors through two more compounding stages.

Using the compound error model: even if each level achieved $\varepsilon = 0.15$ (better than most observed values), five levels would yield $0.85^5 = 0.444$---less than half of queries correctly routed. The observed nDCG of 0.094 suggests actual per-level error is substantially worse at medium scale, consistent with increased difficulty of discriminating among more siblings at each level.

% ============================================================
% 6. DISCUSSION
% ============================================================
\section{Discussion}

\subsection{Why Embedding-Based Routing Fails at Scale}

The mechanism is $(1-\varepsilon)^d$, confirmed empirically. Three factors combine to make this fatal at scale:

\textbf{Depth increases with corpus size.} A tree with branching factor~8 requires depth~3 for 315 chunks but depth~5 for 21,897 chunks. Each additional level multiplies the cumulative error. There is no way to avoid this while maintaining logarithmic scoring cost.

\textbf{Per-level epsilon does not improve with scale.} Embedding cosine similarity between a query and a structured routing summary is a function of the summary's quality and the embedding model's capacity. Adding more chunks to the corpus makes each internal node's children more numerous and more similar, \emph{increasing} $\varepsilon$ at each level. The problem gets harder, not easier, with scale.

\textbf{The interaction is multiplicative, not additive.} A system that is ``pretty good'' at routing ($\varepsilon = 0.15$ per level) is catastrophic at depth~5: $0.85^5 = 0.444$ recall before any leaf-level errors. At $\varepsilon = 0.25$: $0.75^5 = 0.237$. The exponential decay is unforgiving.

This explains why the small-scale results were misleadingly optimistic. At depth~3 with the best configuration ($\varepsilon_1 = 0.06$, $\varepsilon_2 = 0.28$), the compounding is manageable. Adding two levels transforms a viable system into a non-functional one.

\subsection{Five Empirical Patterns}
\label{sec:patterns}

Our systematic exploration of 12 configurations revealed five patterns that explain specific failure mechanisms.

\textbf{Pattern 1: DPI Summary Blindness.} Routing summaries, by design, compress information. The Data Processing Inequality guarantees they lose detail. Queries targeting specific facts (dates, version numbers, conditional logic, quantities) score poorly against thematic summaries because the discriminative features are absent. The structured summary format (key\_entities, key\_terms) partially mitigates this but cannot recover information that was never salient enough to include.

\textbf{Pattern 2: Cross-Encoder Routing Damage.} The MS-MARCO cross-encoder, trained on natural language query-passage pairs, is poorly calibrated for structured routing metadata. When applied to internal node scoring (v2), it flipped 26 correct cosine routing decisions to incorrect while correcting only 14---a net loss. Routing summaries (structured lists of topics, entities, and keywords) are distributionally different from MS-MARCO training data. Cross-encoder scoring remains effective at the leaf level, where it operates on actual chunk text.

\textbf{Pattern 3: Beam Width as Diagnostic.} Increasing beam width from 3 to 8 monotonically improved both nDCG and per-level epsilon across all configurations. This is diagnostic: if the tree structure were fundamentally broken (correct branches did not exist), wider beams would not help. The monotonic improvement confirms that the tree contains the right information in the right places---the routing mechanism simply cannot find it reliably.

\textbf{Pattern 4: Embedding Saturation.} Three approaches to improving summary embeddings (enriched text, contrastive prompts, content snippets) all plateaued at similar nDCG levels (${\sim}$0.48--0.49) with MiniLM (384-dim). Switching to mpnet (768-dim) with a rebuilt tree improved nDCG to 0.540, but the gains came primarily from better \emph{leaf-level} discrimination (L4\,$\varepsilon$ improved from 0.81 to 0.75), not from routing improvement (L1\,$\varepsilon$ actually worsened from 0.16 to 0.24). Routing quality improvement requires a different lever than embedding dimensionality.

\textbf{Pattern 5: BM25 Routing Sparsity.} Hybrid BM25+cosine routing via reciprocal rank fusion (v12) degraded nDCG by 14\% compared to cosine-only routing (v10). BM25 operates on routing summaries containing only ${\sim}$10--20 tokens of keywords per node. This is too sparse for meaningful lexical matching, producing near-random BM25 scores that inject noise into the fusion.

\subsection{Implications for Hierarchical Retrieval}

\textbf{The tree structure is not the problem.} Beam width sweep (Pattern~3) confirms that correct branches exist at every level. Sibling distinctiveness passes at both scales (0.608 small, 0.445 medium).

\textbf{The routing mechanism is the problem.} Embedding cosine similarity between a query and a structured routing summary is fundamentally limited in discriminative power. The summary embedding must simultaneously encode topic scope, entity coverage, keyword relevance, and contrastive exclusions in a single vector.

\textbf{RAPTOR's design insight is validated.} RAPTOR's collapsed-tree approach---flat retrieval over nodes at all levels, avoiding top-down routing entirely---sidesteps the error compounding problem. Our results provide a quantitative explanation for \emph{why} collapsed tree outperforms tree traversal.

\textbf{Per-level epsilon should be standard.} Any hierarchical retrieval system making sequential routing decisions through a tree should report per-level epsilon alongside end-to-end metrics. Without it, there is no way to determine whether poor performance is due to tree structure, routing mechanism, or leaf-level retrieval.

% ============================================================
% 7. CONCLUSION
% ============================================================
\section{Conclusion}

We built HCR to test whether embedding-based routing through LLM-generated summaries could make hierarchical retrieval practical under token constraints. The answer is no. At 21,897 chunks, HCR achieved nDCG@10 of 0.094 versus 0.749 for flat retrieval with cross-encoder reranking---a catastrophic failure caused by exponential compounding of per-level routing error across five tree levels.

The per-level routing epsilon metric we introduce reveals exactly where and why this failure occurs. At each level, embedding cosine similarity against structured routing summaries makes moderate errors ($\varepsilon \approx 0.15$--$0.30$ at the best configurations). These errors compound multiplicatively with depth, producing near-random leaf selection at the scales where hierarchical retrieval is most needed.

Three points deserve emphasis:

\begin{enumerate}
    \item \textbf{The tree structure works.} Beam width sweep confirms that correct branches exist at every level. Content is well-organized; routing cannot find it.

    \item \textbf{The routing mechanism fails.} Embedding-based cosine similarity on structured summaries is not discriminative enough for reliable multi-level routing. This is a fundamental limitation, not a tuning problem---we tested four routing strategies, two embedding models, and three summary representations.

    \item \textbf{Per-level epsilon is necessary.} Without per-level measurement, the small-scale nDCG of 0.580 might have appeared promising. Per-level epsilon revealed that this performance was supported by shallow depth (3~levels), not by routing accuracy, predicting the collapse at medium scale.
\end{enumerate}

Future work should decouple tree construction (which works) from routing (which does not). Promising directions include learned routing functions trained specifically on tree-structured retrieval decisions, LLM-as-judge routing (as in LATTICE, though at higher cost), or hybrid architectures that use the tree for representation enrichment while avoiding strict top-down routing for final retrieval.

% ============================================================
% REFERENCES
% ============================================================
\begin{thebibliography}{11}

\bibitem{raptor}
P.~Sarthi, S.~Abdullah, A.~Tuli, S.~Khanna, A.~Goldie, and C.D.~Manning.
\newblock RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval.
\newblock In \emph{ICLR}, 2024.

\bibitem{lattice}
N.~Gupta, W.-C.~Chang, N.~Bui, C.-J.~Hsieh, and I.S.~Dhillon.
\newblock LLM-guided Hierarchical Retrieval.
\newblock \emph{arXiv:2510.13217}, 2025.

\bibitem{cover2006}
T.M.~Cover and J.A.~Thomas.
\newblock \emph{Elements of Information Theory}.
\newblock Wiley-Interscience, 2nd edition, 2006.

\bibitem{hiro}
A.~Goel and A.~Chandak.
\newblock HIRO: Hierarchical Information Retrieval Optimization.
\newblock \emph{arXiv:2406.09979}, 2024.

\bibitem{graphrag}
Microsoft.
\newblock GraphRAG: Graph-based Retrieval-Augmented Generation, 2024.

\bibitem{nogueira2019}
R.~Nogueira and K.~Cho.
\newblock Passage Re-ranking with BERT.
\newblock \emph{arXiv:1901.04085}, 2019.

\bibitem{vanrijsbergen1979}
C.J.~van Rijsbergen.
\newblock \emph{Information Retrieval}.
\newblock Butterworths, 2nd edition, 1979.

\bibitem{voorhees1985}
E.M.~Voorhees.
\newblock The cluster hypothesis revisited.
\newblock In \emph{Proceedings of SIGIR}, 1985.

\end{thebibliography}

% ============================================================
% APPENDICES
% ============================================================
\appendix

\section{Full Configuration Table}
\label{app:configs}

All 12 configurations with complete metrics. v1 discarded due to tree builder bug.

\begin{table}[h]
\centering
\caption{Complete HCR configuration results (small corpus, 315 chunks, 50 queries)}
\label{tab:full-configs}
\footnotesize
\begin{tabular}{lccllccccccc}
\toprule
\textbf{Cfg} & \textbf{Bm} & \textbf{Dims} & \textbf{Tree} & \textbf{Route} & \textbf{Text} & \textbf{nDCG} & \textbf{R@10} & \textbf{MRR} & \textbf{Tok} & \textbf{L1\,$\varepsilon$} & \textbf{L2\,$\varepsilon$} \\
\midrule
v1 & 3 & 384 & MiniLM & CE all & Def & 0.345 & --- & --- & --- & --- & --- \\
v2 & 3 & 384 & MiniLM & CE all & Def & 0.318 & 0.40 & 0.298 & 234 & 0.32 & 0.62 \\
v3 & 3 & 384 & MiniLM & Cosine & Def & 0.287 & 0.32 & 0.281 & 235 & 0.32 & 0.58 \\
v4 & 5 & 384 & MiniLM & Cosine & Def & 0.420 & 0.46 & 0.411 & 255 & 0.24 & 0.46 \\
v5 & 8 & 384 & MiniLM & Cosine & Def & 0.509 & 0.58 & 0.490 & 297 & 0.14 & 0.36 \\
v6 & 5 & 384 & MiniLM & Cosine & Enr & 0.493 & 0.54 & --- & 249 & 0.16 & 0.36 \\
v7 & 5 & 384 & MiniLM & Cosine & Con & 0.484 & --- & --- & 269 & 0.14 & 0.44 \\
v8 & 5 & 384 & MiniLM & Cosine & Snp & 0.485 & --- & --- & 279 & 0.16 & 0.42 \\
v9 & 5 & 768 & MiniLM & Cosine & Enr & 0.450 & 0.54 & --- & 296 & 0.18 & 0.38 \\
v10 & 5 & 768 & mpnet & Cosine & Enr & 0.540 & 0.62 & 0.516 & 231 & 0.24 & 0.42 \\
v11 & 8 & 768 & mpnet & Cosine & Enr & \textbf{0.580} & \textbf{0.66} & \textbf{0.556} & 290 & \textbf{0.06} & \textbf{0.28} \\
v12 & 5 & 768 & mpnet & BM25+ & Enr & 0.465 & --- & --- & --- & --- & --- \\
\bottomrule
\end{tabular}
\begin{flushleft}
\footnotesize
Bm = beam width; Dims = embedding dimensions; Text: Def = default, Enr = enriched, Con = contrastive, Snp = snippets. ``---'' = not recorded. v1 discarded (tree builder bug). v9 uses mpnet embeddings on a MiniLM-clustered tree (misaligned).
\end{flushleft}
\end{table}

\section{Per-Category Breakdown (Small Scale)}
\label{app:categories}

\paragraph{Baseline performance by category.}

\begin{table}[h]
\centering
\caption{Flat+CE baseline performance by query category (small corpus)}
\label{tab:baseline-cats}
\begin{tabular}{lccc}
\toprule
\textbf{Category} & \textbf{N} & \textbf{CE nDCG@10} & \textbf{CE--BM25 Gap} \\
\midrule
ambiguous & 2 & 0.565 & +0.315 \\
dpi & 7 & 0.615 & +0.006 \\
comparative & 5 & 0.663 & +0.451 \\
single\_branch & 12 & 0.792 & +0.023 \\
entity\_spanning & 10 & 1.000 & +0.137 \\
multi\_hop & 5 & 1.000 & +0.274 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{HCR performance by category} (v12 configuration).

\begin{table}[h]
\centering
\caption{HCR performance by query category (small corpus, v12)}
\label{tab:hcr-cats}
\begin{tabular}{lcccc}
\toprule
\textbf{Category} & \textbf{N} & \textbf{nDCG@10} & \textbf{R@10} & \textbf{Notes} \\
\midrule
single\_branch & 12 & 0.244 & 0.33 & 8/12 scored 0.0 \\
entity\_spanning & 10 & 0.663 & 0.70 & Best HCR category \\
dpi & 7 & 0.574 & 0.71 & Bimodal \\
multi\_hop & 5 & 0.400 & 0.40 & 2/5 perfect, 3/5 zero \\
comparative & 5 & 0.077 & 0.20 & Near-complete failure \\
aggregation & 2 & 0.500 & 0.50 & N too small \\
temporal & 2 & 1.000 & 1.00 & Perfect (N=2) \\
ambiguous & 2 & 0.816 & 1.00 & Strong (N=2) \\
ood & 5 & 0.526 & 0.60 & Mixed \\
\bottomrule
\end{tabular}
\end{table}

HCR's failure is concentrated in single\_branch and comparative queries. Single\_branch queries should be the easiest for hierarchical retrieval, yet HCR fails on 67\% of them---indicating routing errors at early levels misdirect the beam.

\section{Routing Diagnosis: Cosine vs Cross-Encoder}
\label{app:routing}

Configuration v2 applied the MS-MARCO cross-encoder at all tree levels. Configuration v3 used cosine similarity only.

\begin{table}[h]
\centering
\caption{Cosine vs cross-encoder routing comparison}
\label{tab:ce-diagnosis}
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{v2 (CE all)} & \textbf{v3 (Cosine)} & \textbf{Delta} \\
\midrule
nDCG@10 & 0.318 & 0.287 & +0.031 \\
Recall@10 & 0.40 & 0.32 & +0.08 \\
L1\,$\varepsilon$ & 0.32 & 0.32 & 0.00 \\
L2\,$\varepsilon$ & 0.62 & 0.58 & +0.04 \\
\bottomrule
\end{tabular}
\end{table}

Detailed analysis of L1 routing decisions revealed 26 queries where cosine was correct but CE flipped to incorrect, and 14 where CE corrected cosine---a net loss of 12 correct decisions. The MS-MARCO cross-encoder assigns unreliable scores to structured routing metadata, which is distributionally different from its training data (natural language query-passage pairs). Example routing summary format:

\begin{lstlisting}
Theme: GitLab procurement and vendor management
Covers: purchase orders, vendor onboarding, Coupa system
Not: engineering practices, HR policies, product development
Entities: Coupa, ZIP, Navan, GitLab Procurement
Terms: requisition, PO, vendor, procurement, contract
\end{lstlisting}

\textbf{Design implication:} Cross-encoder reranking should be restricted to leaf-level scoring, where it operates on natural language chunk text matching its training distribution.

\end{document}
