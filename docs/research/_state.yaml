# Research State
# Machine-readable current state for session continuity
#
# Update this at session end and read at session start.

version: "1.0"
last_updated: "2026-02-16"
last_session: "2026-02-16"

# Current focus
current:
  phase: "validation"  # discovery | research | implementation | validation
  focus: "Summary quality inspector complete. Root cause diagnosed: cross-encoder is NET NEGATIVE for routing (hurts 26 decisions, saves 14). CE trained on natural language, not structured routing metadata. Next: implement cosine-only routing for internal nodes (quick win), then improve summary content."
  blockers: []

# Research briefs status
briefs:
  active: []
  completed:
    - id: "RB-001"
      title: "Prior art survey"
      completed: "2026-02-13"
      key_finding: "12+ systems in the space since 2024. Hierarchical retrieval outperforms flat RAG on complex tasks (2-20pp gains). LATTICE is closest to HCR. No system targets hard token budgets. RAPTOR collapsed-tree result challenges strict top-down traversal."
    - id: "RB-002"
      title: "Theoretical basis: elimination vs similarity"
      completed: "2026-02-13"
      key_finding: "Strict top-down elimination is theoretically fragile — error compounds as (1-ε)^d. RAPTOR collapsed-tree result is structurally predicted, not anomalous. Hybrid coarse-to-fine (shallow elimination + flat search within survivors) is theoretically optimal. H1 needs reframing from 'elimination > similarity' to 'hierarchical coarse-to-fine with token budgets > flat retrieval'."
    - id: "RB-003"
      title: "Scoring mechanics"
      completed: "2026-02-13"
      key_finding: "Cascade architecture (hybrid BM25+dense → cross-encoder) achieves ε ≈ 0.01–0.02 per level at ~40–80ms. Strict admissibility impossible for semantic relevance. Path-relevance EMA is higher leverage than per-node scoring. Summary quality is #1 upstream factor. AdaGReS submodular knapsack for token-budget selection. H1c → 75%."
    - id: "RB-004"
      title: "Tree construction"
      completed: "2026-02-13"
      key_finding: "Four-source convergence on construction recipe: top-down divisive clustering (bisecting k-means, d=2–3, b∈[6,15]) + LLM contrastive routing summaries + soft assignment (1–3 parents per leaf). Routing summaries are a distinct artifact class: {theme, includes, excludes, key_entities, key_terms}. Gemini adds Q-STRUM Debate for adversarial contrastive generation, depth-variable summary formatting (contrastive prose at L1, keyword arrays + multi-vector at L2), Schema Entropy Bound and Sibling Distinctiveness (BM25 softmax) as concrete tree quality metrics, and EraRAG LSH hyperplanes as alternative partitioning with deterministic incremental maintenance. Cross-branch defense requires five layers. No routing-specific tree quality metric in literature — genuine research gap. H1b → 80%."
    - id: "RB-005"
      title: "Failure modes"
      completed: "2026-02-13"
      key_finding: "No architectural showstopper. 26 failure modes identified across all pipeline stages. Overall expected failure rate: 10–20%. Top residual risks: (1) DPI information loss for detail queries, (2) budget impossibility for aggregation/listing queries, (3) beam collapse without diversity enforcement. Three design changes needed: beam diversity, collapsed-tree as co-primary, external source handling. Entity cross-links elevated to primary mechanism for dominant query type (entity-centric). Transition period (small corpus) is highest-risk deployment phase."
    - id: "RB-006"
      title: "Benchmark design"
      completed: "2026-02-13"
      key_finding: "Four-source convergence on benchmark design. Hybrid corpus (50K–100K chunks from GitLab handbook + EnronQA + synthetic injectors). 300–400 stratified queries with budget-feasibility labels. 7 core metrics led by per-level routing accuracy ε (never measured in any system) and answer sufficiency under token constraint. 4 baselines (BM25, hybrid flat, flat+CE as kill baseline, RAPTOR). Fail-fast sequence with kill criteria at each step. MVB costs $15–30 in LLM API calls. Success criteria: HCR@400 ≥ flat@400 + 5pp, ε ≤ 0.03 per level, dual-path ≥ single-path + 3pp. Kill: flat+CE beats HCR at full corpus with significance."

# Baseline results (2026-02-15)
baseline_results:
  bm25:
    ndcg_at_10: 0.705
    recall_at_10: 0.82
    mrr: 0.669
    mean_tokens: 333
  hybrid_rrf:
    ndcg_at_10: 0.719
    recall_at_10: 0.90
    mrr: 0.662
    mean_tokens: 343
  flat_ce:
    ndcg_at_10: 0.835
    recall_at_10: 0.94
    mrr: 0.803
    mean_tokens: 354
  notes: "IR metrics computed on full ranked list (top-50). Token metrics on 400-token packed result. Median chunk ~470 tokens, so packing yields 0-1 chunks per query. Flat+CE is kill baseline. Results in benchmark/results/baseline_results.json (gitignored)."

# HCR results
hcr_results:
  config_v1:
    tree_depth: 2
    branching: 10
    beam_width: 3
    actual_branches: 4
    ndcg_at_10: 0.345
    recall_at_10: 0.36
    mrr: 0.340
    mean_tokens: 161
    epsilon_level_0: 0.000
    epsilon_level_1: 0.460
    epsilon_level_2: 0.640
    sibling_distinctiveness: 0.690
    notes: "OBSOLETE — tree builder was broken (flat, not hierarchical). See config_v2."
  config_v2:
    tree_depth: 3
    branching: 8
    beam_width: 3
    tree_structure: "L0:1(8) L1:8(8) L2:64(avg4.7) L3:253 L4:80 leaves"
    ndcg_at_10: 0.318
    recall_at_10: 0.40
    mrr: 0.298
    mean_tokens: 234
    epsilon_level_0: 0.000
    epsilon_level_1: 0.320
    epsilon_level_2: 0.620
    epsilon_level_3: 0.780
    epsilon_level_4: 1.000
    sibling_distinctiveness: 0.608
    notes: "Fixed hierarchical builder. Proper k-ary tree. L1 epsilon improved 0.46→0.32 but still 10x off target. Problem is summary quality, not tree shape."

# Routing diagnosis (2026-02-16, session 2)
routing_diagnosis:
  inspector_script: "scripts/inspect_summaries.py"
  per_level_accuracy:
    L0_cosine_miss: "24%"
    L0_ce_miss: "14%"
    L1_cosine_miss: "22%"
    L1_ce_miss: "22%"
    L2_cosine_miss: "8%"
    L2_ce_miss: "38%"
    L3_cosine_miss: "0%"
    L3_ce_miss: "21%"
  decision_breakdown:
    total_decisions: 162
    both_correct: "109 (67%)"
    cosine_ok_ce_wrong: "26 (16%)"
    cosine_wrong_ce_ok: "14 (9%)"
    both_wrong: "13 (8%)"
  ce_score_distribution:
    when_correct_mean: -3.833
    when_correct_range: "-11.09 to +5.69"
    when_incorrect_correct_branch_mean: -7.067
    when_incorrect_top1_mean: -4.767
  root_cause: |
    MS-MARCO cross-encoder trained on natural language passages, not structured
    routing metadata ("Theme: X. Includes: A, B. Excludes: C."). Scores deeply
    negative for all candidates, destroying signal. CE is net negative: hurts
    26 correct cosine decisions, saves only 14. At L2 especially destructive
    (8% cosine miss → 38% CE miss).
  category_failure_rates:
    temporal: "2/2 (100%)"
    ambiguous: "2/2 (100%)"
    single_branch: "10/12 (83%)"
    dpi: "5/7 (71%)"
    comparative: "3/5 (60%)"
    ood: "3/5 (60%)"
    entity_spanning: "4/10 (40%)"
    multi_hop: "2/5 (40%)"
    aggregation: "0/2 (0%)"

# Implementation phases completed
implementation:
  phases_complete:
    - "Phase 1: Core types (Pydantic models)"
    - "Phase 2: Corpus pipeline (chunker, loader, embedder)"
    - "Phase 3: Index infrastructure (BM25, vector, hybrid)"
    - "Phase 4: LLM client + caching"
    - "Phase 5: Retrieval baselines (BM25, hybrid, flat+CE)"
    - "Phase 6: Evaluation metrics (epsilon, sufficiency, IR)"
    - "Phase 7: Query suite + benchmark runner"
    - "Phase 9: HCR core (tree builder, beam traversal, collapsed retrieval, dual-path, scoring cascade)"
    - "Code review fixes"
    - "Query generator fix (JSON fence stripping, Haiku model)"
    - "Sanity check passed (all baselines operational)"
    - "Baseline evaluation fix (separate ranking from token packing)"
    - "Baseline evaluation complete (BM25, hybrid, flat+CE)"
    - "Per-category baseline analysis (scripts/analyse_baselines.py)"
    - "HCR baseline class + benchmark integration"
    - "Cascade leaf scoring fix (use chunk embeddings for leaves)"
    - "First HCR evaluation: nDCG=0.345, epsilon L1=0.46"
    - "Hierarchical tree builder fix (k-ary clustering + recursive node creation)"
    - "Config A evaluation: nDCG=0.318, epsilon L1=0.32 (proper tree)"
    - "Summary quality inspector (scripts/inspect_summaries.py)"
    - "Routing diagnosis: CE is net negative for routing, cosine-only is better"
  test_count: 126
  mypy_status: "clean (30 source files)"
  ruff_status: "clean"

# Hypothesis summary
hypotheses:
  total: 3
  validated: 0
  invalidated: 0
  uncertain: 3
  retired: 1  # Original H1
  highest_confidence: "H1b (hybrid coarse-to-fine superiority) at 80%"
  lowest_confidence: "H1a (token efficiency) at 65%"
  notes: |
    Two HCR configs tested. Both show epsilon L1 >> 0.03 target.
    Tree shape is not the primary lever — summary discriminativeness is.
    Token efficiency confirmed (161-234 vs 354 baseline).
    Hypothesis validation blocked on routing quality.
    NEW: Cross-encoder identified as net negative for routing.
    Cosine-only routing should be tested as immediate improvement.

# Knowledge accumulation
knowledge:
  patterns_discovered: 1
  variants_discovered: 0
  contexts_documented: 0
  domains_documented: 0
  edge_cases: 0
  notes: |
    Pattern 1: "DPI summary blindness" — cross-encoder gives all branches
    ~-11 score for specific detail queries because coarse routing summaries
    don't contain the specific terms. Predicted by RB-005 failure mode #1.
    Confirmed across both tree configs (flat and hierarchical).
    Pattern 2: "CE routing damage" — MS-MARCO CE trained on natural language,
    not structured metadata. Scores deeply negative (-3 to -11) even for correct
    branch. Flips 26 correct cosine decisions to wrong. Net negative for routing.

# Next actions (prioritized)
next_actions:
  - id: 1
    action: "QUICK WIN: Cosine-only routing for internal nodes"
    details: |
      In hcr_core/scoring/cascade.py, skip cross-encoder (stage 2) when scoring
      internal nodes (nodes with summaries). Keep CE only for leaf-level scoring
      where it sees actual chunk content. This should flip L2 from 38% CE miss
      back to 8% cosine miss — a 4.75x improvement with one change.
    files:
      - "hcr_core/scoring/cascade.py — add skip_ce_for_summaries option"
    test: "Re-run scripts/inspect_summaries.py --brief to verify improvement"
  - id: 2
    action: "Re-run HCR benchmark with cosine-only routing"
    details: |
      Delete benchmark/results/hcr_tree.json is NOT needed (tree unchanged).
      Just re-run evaluation with cosine-only cascade.
      Run: set -a && source .env && set +a && python scripts/run_benchmark.py --mode hcr
    test: "Compare epsilon and nDCG against Config A (v2) results"
  - id: 3
    action: "Improve summary content (if cosine-only still insufficient)"
    details: |
      Three improvements to summarizer.py:
      a) Include key_entities and key_terms in the text fed to scoring
         (currently cascade._get_text only uses theme/includes/excludes)
      b) Add sample content snippets to summaries for better term coverage
      c) Stronger contrastive prompts (force specific differentiators)
      Each change requires tree rebuild: delete benchmark/results/hcr_tree.json
    files:
      - "hcr_core/tree/summarizer.py — prompt and summary generation"
      - "hcr_core/scoring/cascade.py — _get_text() format"
  - id: 4
    action: "If epsilon improves to <0.10: widen beam to 5, compare nDCG"
    details: "Beam width in scripts/run_benchmark.py line ~375 (HCRBaseline init)"

# Notes for next session
notes: |
  SESSION 2026-02-16 (session 2) SUMMARY:

  BUILT: Summary quality inspector (scripts/inspect_summaries.py)
  Traces each query through tree showing cosine and CE scores at every level,
  with correct branch vs chosen branch comparison.

  CRITICAL FINDING: Cross-encoder is NET NEGATIVE for routing.
  - 162 total routing decisions across 50 queries, 4 levels
  - CE flips 26 correct cosine decisions to WRONG (16%)
  - CE saves only 14 cosine misses (9%)
  - At L2: cosine gets 92% correct, CE drops it to 62%
  - CE scores are deeply negative (-3 to -11) because MS-MARCO was trained
    on natural language passages, not structured routing metadata
  - Mean CE score even when correct: -3.833 (range -11 to +5.7)

  THREE PROBLEMS IDENTIFIED (severity order):
  1. CE is net negative for routing — biggest single lever
  2. Summary content too abstract (no entities/terms in CE text format)
  3. Root-level routing weak (24% cosine miss at L0)

  QUICK WIN for next session: cosine-only routing for internal nodes.
  Skip CE stage 2 in cascade.py when scoring nodes with summaries.
  Keep CE only for leaf-level scoring where it sees actual chunk content.
  Expected: L2 miss drops from 38% to 8%. Should significantly improve
  nDCG from 0.318.

  Inspector command: scripts/inspect_summaries.py
    --brief          aggregate only
    --failures-only  only misrouted queries
    --top-n N        worst N failures
    --level L        focus on level L failures

  Key files:
  - hcr_core/scoring/cascade.py — the cascade with CE (modify here first)
  - hcr_core/tree/summarizer.py — prompt and summary generation
  - hcr_core/tree/builder.py — calls summarizer per node
  - scripts/inspect_summaries.py — routing quality inspector
  - scripts/run_benchmark.py — benchmark runner

  Run: set -a && source .env && set +a && python scripts/run_benchmark.py --mode hcr
  Inspector: set -a && source .env && set +a && python scripts/inspect_summaries.py --brief
  Branch: 0215jc
