# Research State
# Machine-readable current state for session continuity
#
# Update this at session end and read at session start.

version: "1.0"
last_updated: "2026-02-16"
last_session: "2026-02-16"

# Current focus
current:
  phase: "validation"  # discovery | research | implementation | validation
  focus: "Tree builder fixed (was flat, now hierarchical). Config A tested (depth=3, branching=8): nDCG=0.318, epsilon L1=0.32. Tree shape is no longer the bottleneck — summary quality is. Next: improve routing summary discriminativeness."
  blockers: []

# Research briefs status
briefs:
  active: []
  completed:
    - id: "RB-001"
      title: "Prior art survey"
      completed: "2026-02-13"
      key_finding: "12+ systems in the space since 2024. Hierarchical retrieval outperforms flat RAG on complex tasks (2-20pp gains). LATTICE is closest to HCR. No system targets hard token budgets. RAPTOR collapsed-tree result challenges strict top-down traversal."
    - id: "RB-002"
      title: "Theoretical basis: elimination vs similarity"
      completed: "2026-02-13"
      key_finding: "Strict top-down elimination is theoretically fragile — error compounds as (1-ε)^d. RAPTOR collapsed-tree result is structurally predicted, not anomalous. Hybrid coarse-to-fine (shallow elimination + flat search within survivors) is theoretically optimal. H1 needs reframing from 'elimination > similarity' to 'hierarchical coarse-to-fine with token budgets > flat retrieval'."
    - id: "RB-003"
      title: "Scoring mechanics"
      completed: "2026-02-13"
      key_finding: "Cascade architecture (hybrid BM25+dense → cross-encoder) achieves ε ≈ 0.01–0.02 per level at ~40–80ms. Strict admissibility impossible for semantic relevance. Path-relevance EMA is higher leverage than per-node scoring. Summary quality is #1 upstream factor. AdaGReS submodular knapsack for token-budget selection. H1c → 75%."
    - id: "RB-004"
      title: "Tree construction"
      completed: "2026-02-13"
      key_finding: "Four-source convergence on construction recipe: top-down divisive clustering (bisecting k-means, d=2–3, b∈[6,15]) + LLM contrastive routing summaries + soft assignment (1–3 parents per leaf). Routing summaries are a distinct artifact class: {theme, includes, excludes, key_entities, key_terms}. Gemini adds Q-STRUM Debate for adversarial contrastive generation, depth-variable summary formatting (contrastive prose at L1, keyword arrays + multi-vector at L2), Schema Entropy Bound and Sibling Distinctiveness (BM25 softmax) as concrete tree quality metrics, and EraRAG LSH hyperplanes as alternative partitioning with deterministic incremental maintenance. Cross-branch defense requires five layers. No routing-specific tree quality metric in literature — genuine research gap. H1b → 80%."
    - id: "RB-005"
      title: "Failure modes"
      completed: "2026-02-13"
      key_finding: "No architectural showstopper. 26 failure modes identified across all pipeline stages. Overall expected failure rate: 10–20%. Top residual risks: (1) DPI information loss for detail queries, (2) budget impossibility for aggregation/listing queries, (3) beam collapse without diversity enforcement. Three design changes needed: beam diversity, collapsed-tree as co-primary, external source handling. Entity cross-links elevated to primary mechanism for dominant query type (entity-centric). Transition period (small corpus) is highest-risk deployment phase."
    - id: "RB-006"
      title: "Benchmark design"
      completed: "2026-02-13"
      key_finding: "Four-source convergence on benchmark design. Hybrid corpus (50K–100K chunks from GitLab handbook + EnronQA + synthetic injectors). 300–400 stratified queries with budget-feasibility labels. 7 core metrics led by per-level routing accuracy ε (never measured in any system) and answer sufficiency under token constraint. 4 baselines (BM25, hybrid flat, flat+CE as kill baseline, RAPTOR). Fail-fast sequence with kill criteria at each step. MVB costs $15–30 in LLM API calls. Success criteria: HCR@400 ≥ flat@400 + 5pp, ε ≤ 0.03 per level, dual-path ≥ single-path + 3pp. Kill: flat+CE beats HCR at full corpus with significance."

# Baseline results (2026-02-15)
baseline_results:
  bm25:
    ndcg_at_10: 0.705
    recall_at_10: 0.82
    mrr: 0.669
    mean_tokens: 333
  hybrid_rrf:
    ndcg_at_10: 0.719
    recall_at_10: 0.90
    mrr: 0.662
    mean_tokens: 343
  flat_ce:
    ndcg_at_10: 0.835
    recall_at_10: 0.94
    mrr: 0.803
    mean_tokens: 354
  notes: "IR metrics computed on full ranked list (top-50). Token metrics on 400-token packed result. Median chunk ~470 tokens, so packing yields 0-1 chunks per query. Flat+CE is kill baseline. Results in benchmark/results/baseline_results.json (gitignored)."

# HCR results
hcr_results:
  config_v1:
    tree_depth: 2
    branching: 10
    beam_width: 3
    actual_branches: 4
    ndcg_at_10: 0.345
    recall_at_10: 0.36
    mrr: 0.340
    mean_tokens: 161
    epsilon_level_0: 0.000
    epsilon_level_1: 0.460
    epsilon_level_2: 0.640
    sibling_distinctiveness: 0.690
    notes: "OBSOLETE — tree builder was broken (flat, not hierarchical). See config_v2."
  config_v2:
    tree_depth: 3
    branching: 8
    beam_width: 3
    tree_structure: "L0:1(8) L1:8(8) L2:64(avg4.7) L3:253 L4:80 leaves"
    ndcg_at_10: 0.318
    recall_at_10: 0.40
    mrr: 0.298
    mean_tokens: 234
    epsilon_level_0: 0.000
    epsilon_level_1: 0.320
    epsilon_level_2: 0.620
    epsilon_level_3: 0.780
    epsilon_level_4: 1.000
    sibling_distinctiveness: 0.608
    notes: "Fixed hierarchical builder. Proper k-ary tree. L1 epsilon improved 0.46→0.32 but still 10x off target. Problem is summary quality, not tree shape."

# Implementation phases completed
implementation:
  phases_complete:
    - "Phase 1: Core types (Pydantic models)"
    - "Phase 2: Corpus pipeline (chunker, loader, embedder)"
    - "Phase 3: Index infrastructure (BM25, vector, hybrid)"
    - "Phase 4: LLM client + caching"
    - "Phase 5: Retrieval baselines (BM25, hybrid, flat+CE)"
    - "Phase 6: Evaluation metrics (epsilon, sufficiency, IR)"
    - "Phase 7: Query suite + benchmark runner"
    - "Phase 9: HCR core (tree builder, beam traversal, collapsed retrieval, dual-path, scoring cascade)"
    - "Code review fixes"
    - "Query generator fix (JSON fence stripping, Haiku model)"
    - "Sanity check passed (all baselines operational)"
    - "Baseline evaluation fix (separate ranking from token packing)"
    - "Baseline evaluation complete (BM25, hybrid, flat+CE)"
    - "Per-category baseline analysis (scripts/analyse_baselines.py)"
    - "HCR baseline class + benchmark integration"
    - "Cascade leaf scoring fix (use chunk embeddings for leaves)"
    - "First HCR evaluation: nDCG=0.345, epsilon L1=0.46"
    - "Hierarchical tree builder fix (k-ary clustering + recursive node creation)"
    - "Config A evaluation: nDCG=0.318, epsilon L1=0.32 (proper tree)"
  test_count: 126
  mypy_status: "clean (30 source files)"
  ruff_status: "clean"

# Hypothesis summary
hypotheses:
  total: 3
  validated: 0
  invalidated: 0
  uncertain: 3
  retired: 1  # Original H1
  highest_confidence: "H1b (hybrid coarse-to-fine superiority) at 80%"
  lowest_confidence: "H1a (token efficiency) at 65%"
  notes: |
    Two HCR configs tested. Both show epsilon L1 >> 0.03 target.
    Tree shape is not the primary lever — summary discriminativeness is.
    Token efficiency confirmed (161-234 vs 354 baseline).
    Hypothesis validation blocked on routing summary quality.

# Knowledge accumulation
knowledge:
  patterns_discovered: 1
  variants_discovered: 0
  contexts_documented: 0
  domains_documented: 0
  edge_cases: 0
  notes: |
    Pattern: "DPI summary blindness" — cross-encoder gives all branches
    ~-11 score for specific detail queries because coarse routing summaries
    don't contain the specific terms. Predicted by RB-005 failure mode #1.
    Confirmed across both tree configs (flat and hierarchical).

# Next actions (prioritized)
next_actions:
  - "Improve routing summary quality (highest leverage)"
  - "Make summaries more contrastive: force LLM to highlight differentiators between sibling clusters"
  - "Include specific key terms, entities, sample content snippets in summaries"
  - "Consider depth-variable formatting (RB-004): contrastive prose at L1, keyword arrays at deeper levels"
  - "After summary improvement: re-run Config A (depth=3, branching=8) and compare epsilons"
  - "If epsilon still high: try cosine-only routing (skip cross-encoder for routing decisions)"
  - "If epsilon improves: widen beam (5) and compare nDCG against baselines"

# Notes for next session
notes: |
  SESSION 2026-02-16 SUMMARY:

  BUG FIXED: Tree builder was producing flat 2-level trees regardless of
  depth parameter. bisecting_kmeans returned flat cluster list, losing
  hierarchy. Fixed both:
  - clustering.py: k-ary hierarchical_kmeans (ClusterNode dataclass)
  - builder.py: recursive _build_subtree with per-level summaries

  CONFIG A RESULTS (depth=3, branching=8, proper k-ary tree):
  - nDCG@10=0.318 vs flat+CE=0.835 (LOSE by 0.517)
  - Recall@10=0.40 vs flat+CE=0.94
  - Mean tokens: 234 vs 354
  - Epsilon: L0=0.00, L1=0.32, L2=0.62, L3=0.78, L4=1.00
  - Tree: L0:1(8) L1:8(8) L2:64(avg4.7) L3:253 L4:80 leaves

  KEY INSIGHT: Tree shape is not the bottleneck. Both flat (v1) and
  hierarchical (v2) trees fail because routing summaries are too generic.
  The cross-encoder can't distinguish siblings. This is the RB-004 open
  gap: "do contrastive summaries improve routing accuracy?"

  NEXT SESSION TASK: Improve summary quality in summarizer.py.
  Approach:
  1. Make contrastive prompts stronger (force specific differentiators)
  2. Include key terms, entities, sample content snippets
  3. Consider depth-variable formatting (RB-004 Gemini recommendation)
  4. Re-run Config A and measure epsilon improvement

  Key files:
  - hcr_core/tree/summarizer.py — the prompt and summary generation
  - hcr_core/tree/builder.py — calls summarizer per node
  - hcr_core/tree/clustering.py — k-ary hierarchical clustering
  - scripts/run_benchmark.py — depth/branching config at lines ~324-326
  - Delete benchmark/results/hcr_tree.json before each run to rebuild

  Run: set -a && source .env && set +a && python scripts/run_benchmark.py --mode hcr
  Deps: pip install -e '.[dev]'
  Branch: 0215jc
