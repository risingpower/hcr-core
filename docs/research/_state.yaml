# Research State
# Machine-readable current state for session continuity
#
# Update this at session end and read at session start.

version: "1.0"
last_updated: "2026-02-16"
last_session: "2026-02-16"

# Current focus
current:
  phase: "validation"  # discovery | research | implementation | validation
  focus: "Summary text experiments complete (v6-v8). Enriched embedding text gives +17% nDCG. Prompt engineering and content snippets are washes — MiniLM embedding space is saturated at nDCG ~0.49. Next: try stronger embedding model or different architecture approach."
  blockers: []

# Research briefs status
briefs:
  active: []
  completed:
    - id: "RB-001"
      title: "Prior art survey"
      completed: "2026-02-13"
      key_finding: "12+ systems in the space since 2024. Hierarchical retrieval outperforms flat RAG on complex tasks (2-20pp gains). LATTICE is closest to HCR. No system targets hard token budgets. RAPTOR collapsed-tree result challenges strict top-down traversal."
    - id: "RB-002"
      title: "Theoretical basis: elimination vs similarity"
      completed: "2026-02-13"
      key_finding: "Strict top-down elimination is theoretically fragile — error compounds as (1-ε)^d. RAPTOR collapsed-tree result is structurally predicted, not anomalous. Hybrid coarse-to-fine (shallow elimination + flat search within survivors) is theoretically optimal. H1 needs reframing from 'elimination > similarity' to 'hierarchical coarse-to-fine with token budgets > flat retrieval'."
    - id: "RB-003"
      title: "Scoring mechanics"
      completed: "2026-02-13"
      key_finding: "Cascade architecture (hybrid BM25+dense → cross-encoder) achieves ε ≈ 0.01–0.02 per level at ~40–80ms. Strict admissibility impossible for semantic relevance. Path-relevance EMA is higher leverage than per-node scoring. Summary quality is #1 upstream factor. AdaGReS submodular knapsack for token-budget selection. H1c → 75%."
    - id: "RB-004"
      title: "Tree construction"
      completed: "2026-02-13"
      key_finding: "Four-source convergence on construction recipe: top-down divisive clustering (bisecting k-means, d=2–3, b∈[6,15]) + LLM contrastive routing summaries + soft assignment (1–3 parents per leaf). Routing summaries are a distinct artifact class: {theme, includes, excludes, key_entities, key_terms}. Gemini adds Q-STRUM Debate for adversarial contrastive generation, depth-variable summary formatting (contrastive prose at L1, keyword arrays + multi-vector at L2), Schema Entropy Bound and Sibling Distinctiveness (BM25 softmax) as concrete tree quality metrics, and EraRAG LSH hyperplanes as alternative partitioning with deterministic incremental maintenance. Cross-branch defense requires five layers. No routing-specific tree quality metric in literature — genuine research gap. H1b → 80%."
    - id: "RB-005"
      title: "Failure modes"
      completed: "2026-02-13"
      key_finding: "No architectural showstopper. 26 failure modes identified across all pipeline stages. Overall expected failure rate: 10–20%. Top residual risks: (1) DPI information loss for detail queries, (2) budget impossibility for aggregation/listing queries, (3) beam collapse without diversity enforcement. Three design changes needed: beam diversity, collapsed-tree as co-primary, external source handling. Entity cross-links elevated to primary mechanism for dominant query type (entity-centric). Transition period (small corpus) is highest-risk deployment phase."
    - id: "RB-006"
      title: "Benchmark design"
      completed: "2026-02-13"
      key_finding: "Four-source convergence on benchmark design. Hybrid corpus (50K–100K chunks from GitLab handbook + EnronQA + synthetic injectors). 300–400 stratified queries with budget-feasibility labels. 7 core metrics led by per-level routing accuracy ε (never measured in any system) and answer sufficiency under token constraint. 4 baselines (BM25, hybrid flat, flat+CE as kill baseline, RAPTOR). Fail-fast sequence with kill criteria at each step. MVB costs $15–30 in LLM API calls. Success criteria: HCR@400 ≥ flat@400 + 5pp, ε ≤ 0.03 per level, dual-path ≥ single-path + 3pp. Kill: flat+CE beats HCR at full corpus with significance."

# Baseline results (2026-02-15)
baseline_results:
  bm25:
    ndcg_at_10: 0.705
    recall_at_10: 0.82
    mrr: 0.669
    mean_tokens: 333
  hybrid_rrf:
    ndcg_at_10: 0.719
    recall_at_10: 0.90
    mrr: 0.662
    mean_tokens: 343
  flat_ce:
    ndcg_at_10: 0.835
    recall_at_10: 0.94
    mrr: 0.803
    mean_tokens: 354
  notes: "IR metrics computed on full ranked list (top-50). Token metrics on 400-token packed result. Median chunk ~470 tokens, so packing yields 0-1 chunks per query. Flat+CE is kill baseline. Results in benchmark/results/baseline_results.json (gitignored)."

# HCR results
hcr_results:
  config_v1:
    tree_depth: 2
    branching: 10
    beam_width: 3
    actual_branches: 4
    ndcg_at_10: 0.345
    recall_at_10: 0.36
    mrr: 0.340
    mean_tokens: 161
    epsilon_level_0: 0.000
    epsilon_level_1: 0.460
    epsilon_level_2: 0.640
    sibling_distinctiveness: 0.690
    notes: "OBSOLETE — tree builder was broken (flat, not hierarchical). See config_v2."
  config_v2:
    tree_depth: 3
    branching: 8
    beam_width: 3
    cascade: "pre_filter_k=3, final_k=2, CE on all levels"
    tree_structure: "L0:1(8) L1:8(8) L2:64(avg4.7) L3:253 L4:80 leaves"
    ndcg_at_10: 0.318
    recall_at_10: 0.40
    mrr: 0.298
    mean_tokens: 234
    epsilon_level_0: 0.000
    epsilon_level_1: 0.320
    epsilon_level_2: 0.620
    epsilon_level_3: 0.780
    epsilon_level_4: 1.000
    sibling_distinctiveness: 0.608
    notes: "Fixed hierarchical builder. Proper k-ary tree. CE on all levels."
  config_v3_cosine_narrow:
    tree_depth: 3
    branching: 8
    beam_width: 3
    cascade: "pre_filter_k=3, final_k=2, cosine-only for internal nodes"
    ndcg_at_10: 0.287
    recall_at_10: 0.32
    mrr: 0.281
    mean_tokens: 235
    epsilon_level_0: 0.000
    epsilon_level_1: 0.320
    epsilon_level_2: 0.580
    epsilon_level_3: 0.680
    epsilon_level_4: 0.920
    notes: "CE removed from internal node routing. Epsilon improved at L2-L4 but nDCG dropped — CE accidentally helped in 14 decisions lost."
  config_v4_cosine_wide:
    tree_depth: 3
    branching: 8
    beam_width: 5
    cascade: "pre_filter_k=5, final_k=3, cosine-only for internal nodes"
    ndcg_at_10: 0.420
    recall_at_10: 0.46
    mrr: 0.411
    mean_tokens: 255
    epsilon_level_0: 0.000
    epsilon_level_1: 0.240
    epsilon_level_2: 0.460
    epsilon_level_3: 0.540
    epsilon_level_4: 0.879
    notes: "Wider beam. nDCG +32% over v2. Confirms tree structure is sound — correct branches exist but ranked 4th-5th by cosine."
  config_v5_cosine_ceiling:
    tree_depth: 3
    branching: 8
    beam_width: 8
    cascade: "pre_filter_k=8, final_k=5, cosine-only for internal nodes"
    ndcg_at_10: 0.509
    recall_at_10: 0.58
    mrr: 0.490
    mean_tokens: 297
    epsilon_level_0: 0.000
    epsilon_level_1: 0.140
    epsilon_level_2: 0.360
    epsilon_level_3: 0.400
    epsilon_level_4: 0.796
    notes: "Ceiling test (beam=branching at L1). nDCG +60% over v2. L1 ε=0.14 (5x better). Still far from kill baseline (0.835). Token cost still below flat+CE (297 vs 354). Confirms summary quality is the bottleneck, not tree structure."
  config_v6_enriched_embed:
    tree_depth: 3
    branching: 8
    beam_width: 5
    cascade: "pre_filter_k=5, final_k=3, cosine-only internal"
    ndcg_at_10: 0.493
    recall_at_10: 0.54
    mrr: 0.481
    mean_tokens: 249
    epsilon_level_0: 0.000
    epsilon_level_1: 0.160
    epsilon_level_2: 0.360
    epsilon_level_3: 0.480
    epsilon_level_4: 0.813
    sibling_distinctiveness: 0.512
    notes: "Enriched summary_to_text() with all RoutingSummary fields (entities, terms, excludes). No tree rebuild — just re-embedded. +17% nDCG over v4. Best beam=5 result."
  config_v7_contrastive_prompts:
    tree_depth: 3
    branching: 8
    beam_width: 5
    cascade: "pre_filter_k=5, final_k=3, cosine-only internal"
    ndcg_at_10: 0.484
    recall_at_10: 0.52
    mrr: 0.479
    mean_tokens: 269
    epsilon_level_0: 0.000
    epsilon_level_1: 0.140
    epsilon_level_2: 0.440
    epsilon_level_3: 0.500
    epsilon_level_4: 0.784
    sibling_distinctiveness: 0.450
    notes: "More specific contrastive prompts. Rebuilt tree. L1 ε improved marginally but L2 worse. Wash."
  config_v8_content_snippets:
    tree_depth: 3
    branching: 8
    beam_width: 5
    cascade: "pre_filter_k=5, final_k=3, cosine-only internal"
    ndcg_at_10: 0.485
    recall_at_10: 0.54
    mrr: 0.472
    mean_tokens: 279
    epsilon_level_0: 0.000
    epsilon_level_1: 0.160
    epsilon_level_2: 0.420
    epsilon_level_3: 0.500
    epsilon_level_4: 0.778
    sibling_distinctiveness: 0.489
    notes: "Content snippets (first 200 chars) added to summaries. Rebuilt tree. No meaningful improvement. MiniLM embedding space saturated."

# Summary quality experiment (2026-02-16, session 4)
summary_quality_experiment:
  finding: |
    Three approaches tested to improve summary embedding discrimination.
    Only enriched embedding text (v6) helped. Contrastive prompts (v7) and
    content snippets (v8) were washes. All plateau at nDCG ~0.49 with beam=5.
    MiniLM 384-dim embedding space is saturated — adding more text doesn't
    help discrimination. The bottleneck has shifted from summary text quality
    to embedding model capacity.
  best_config: "v6 (enriched embed): nDCG=0.493, L1 ε=0.16, 249 tokens"
  approaches:
    - "v6: Enriched _get_text() with all fields → nDCG=0.493 (+17% vs v4)"
    - "v7: Contrastive prompts, rebuilt tree → nDCG=0.484 (wash)"
    - "v8: Content snippets, rebuilt tree → nDCG=0.485 (wash)"
  conclusion: "Embedding model is the bottleneck, not summary text. Next lever: stronger embedding model (E5-large, BGE, Cohere)."

# Beam width sweep (2026-02-16, session 3)
beam_sweep:
  finding: |
    Wider beam monotonically improves both epsilon and nDCG. Tree structure
    is sound. Correct branches are present but cosine similarity with current
    summary embeddings ranks them too low. beam=8 (ceiling) achieves nDCG=0.509,
    still far from kill baseline 0.835. Summary embedding quality is the #1 lever.
  progression:
    - "beam=3: nDCG=0.287, L1ε=0.32"
    - "beam=5: nDCG=0.420, L1ε=0.24"
    - "beam=8: nDCG=0.509, L1ε=0.14"
  token_efficiency: "beam=8 uses 297 tokens vs flat+CE 354 — token advantage holds even at wide beam"

# Routing diagnosis (2026-02-16, session 2)
routing_diagnosis:
  inspector_script: "scripts/inspect_summaries.py"
  per_level_accuracy:
    L0_cosine_miss: "24%"
    L0_ce_miss: "14%"
    L1_cosine_miss: "22%"
    L1_ce_miss: "22%"
    L2_cosine_miss: "8%"
    L2_ce_miss: "38%"
    L3_cosine_miss: "0%"
    L3_ce_miss: "21%"
  decision_breakdown:
    total_decisions: 162
    both_correct: "109 (67%)"
    cosine_ok_ce_wrong: "26 (16%)"
    cosine_wrong_ce_ok: "14 (9%)"
    both_wrong: "13 (8%)"
  ce_score_distribution:
    when_correct_mean: -3.833
    when_correct_range: "-11.09 to +5.69"
    when_incorrect_correct_branch_mean: -7.067
    when_incorrect_top1_mean: -4.767
  root_cause: |
    MS-MARCO cross-encoder trained on natural language passages, not structured
    routing metadata ("Theme: X. Includes: A, B. Excludes: C."). Scores deeply
    negative for all candidates, destroying signal. CE is net negative: hurts
    26 correct cosine decisions, saves only 14. At L2 especially destructive
    (8% cosine miss → 38% CE miss).
  category_failure_rates:
    temporal: "2/2 (100%)"
    ambiguous: "2/2 (100%)"
    single_branch: "10/12 (83%)"
    dpi: "5/7 (71%)"
    comparative: "3/5 (60%)"
    ood: "3/5 (60%)"
    entity_spanning: "4/10 (40%)"
    multi_hop: "2/5 (40%)"
    aggregation: "0/2 (0%)"

# Implementation phases completed
implementation:
  phases_complete:
    - "Phase 1: Core types (Pydantic models)"
    - "Phase 2: Corpus pipeline (chunker, loader, embedder)"
    - "Phase 3: Index infrastructure (BM25, vector, hybrid)"
    - "Phase 4: LLM client + caching"
    - "Phase 5: Retrieval baselines (BM25, hybrid, flat+CE)"
    - "Phase 6: Evaluation metrics (epsilon, sufficiency, IR)"
    - "Phase 7: Query suite + benchmark runner"
    - "Phase 9: HCR core (tree builder, beam traversal, collapsed retrieval, dual-path, scoring cascade)"
    - "Code review fixes"
    - "Query generator fix (JSON fence stripping, Haiku model)"
    - "Sanity check passed (all baselines operational)"
    - "Baseline evaluation fix (separate ranking from token packing)"
    - "Baseline evaluation complete (BM25, hybrid, flat+CE)"
    - "Per-category baseline analysis (scripts/analyse_baselines.py)"
    - "HCR baseline class + benchmark integration"
    - "Cascade leaf scoring fix (use chunk embeddings for leaves)"
    - "First HCR evaluation: nDCG=0.345, epsilon L1=0.46"
    - "Hierarchical tree builder fix (k-ary clustering + recursive node creation)"
    - "Config A evaluation: nDCG=0.318, epsilon L1=0.32 (proper tree)"
    - "Summary quality inspector (scripts/inspect_summaries.py)"
    - "Routing diagnosis: CE is net negative for routing, cosine-only is better"
    - "Cosine-only routing for internal nodes (cascade.py)"
    - "Beam width sweep: beam=3/5/8, confirming tree structure sound"
    - "Summary text enrichment: summary_to_text() with all RoutingSummary fields"
    - "Contrastive prompts: more specific terms/entities (rebuilt tree)"
    - "Content snippets: first 200 chars in summary embeddings (rebuilt tree)"
    - "Embedding ceiling identified: MiniLM saturated at nDCG ~0.49"
  test_count: 126
  mypy_status: "clean (30 source files)"
  ruff_status: "clean"

# Hypothesis summary
hypotheses:
  total: 3
  validated: 0
  invalidated: 0
  uncertain: 3
  retired: 1  # Original H1
  highest_confidence: "H1b (hybrid coarse-to-fine superiority) at 80%"
  lowest_confidence: "H1a (token efficiency) at 65%"
  notes: |
    Eight HCR configs tested (v1-v8). Beam sweep confirmed tree structure is
    sound. Summary text experiments (v6-v8) showed enriched embedding text
    helps (+17%) but MiniLM embedding space saturates at nDCG ~0.49.
    Best config: v6 (beam=5, enriched embed, nDCG=0.493, 249 tokens).
    Kill baseline: 0.835. Gap likely requires stronger embedding model.
    Token efficiency confirmed (249-297 vs 354 baseline).

# Knowledge accumulation
knowledge:
  patterns_discovered: 4
  variants_discovered: 0
  contexts_documented: 0
  domains_documented: 0
  edge_cases: 0
  notes: |
    Pattern 1: "DPI summary blindness" — cross-encoder gives all branches
    ~-11 score for specific detail queries because coarse routing summaries
    don't contain the specific terms. Predicted by RB-005 failure mode #1.
    Confirmed across both tree configs (flat and hierarchical).
    Pattern 2: "CE routing damage" — MS-MARCO CE trained on natural language,
    not structured metadata. Scores deeply negative (-3 to -11) even for correct
    branch. Flips 26 correct cosine decisions to wrong. Net negative for routing.
    Pattern 3: "Beam width as diagnostic" — monotonic improvement in epsilon/nDCG
    with wider beam confirms tree structure is sound and summary quality is the
    bottleneck. beam=3→5→8 shows clear ceiling curve.
    Pattern 4: "Embedding saturation" — adding more text (entities, terms,
    content snippets) to MiniLM 384-dim embeddings doesn't improve discrimination
    beyond the enriched-embed baseline. Prompts and content don't matter once
    embedding space is the bottleneck. Model capacity is the lever.

# Next actions (prioritized)
next_actions:
  - id: 1
    action: "Try stronger embedding model for routing"
    details: |
      MiniLM (384-dim, all-MiniLM-L6-v2) is saturated at nDCG ~0.49.
      Options (in order of effort):
      a) all-mpnet-base-v2 (768-dim, same SentenceTransformers API) — drop-in
      b) E5-large or BGE-large (1024-dim) — needs adapter for embed API
      c) Cohere embed-v3 (1024-dim) — API call, higher quality but cost
      Only need to re-embed summaries (cached tree has text), no rebuild.
      Target: L1 ε ≤ 0.10 with beam=5, nDCG > 0.60.
  - id: 2
    action: "Investigate L4 epsilon (0.78-0.81 across all configs)"
    details: |
      Leaf-level miss rate is ~80% even at beam=8 and across all summary
      quality improvements. Leaf CE scoring is active — may be hurting.
      Or chunk embeddings are poor at this granularity. Needs diagnosis.
  - id: 3
    action: "Consider architectural pivot if embedding model doesn't help"
    details: |
      If stronger embeddings don't close the gap to 0.835, the hierarchical
      routing approach may have a fundamental ceiling with dense embeddings.
      Potential pivots: BM25 hybrid routing (add keyword matching at each
      level), or collapsed-tree-first with beam verification.

# Notes for next session
notes: |
  SESSION 2026-02-16 (session 4) SUMMARY:

  THREE SUMMARY TEXT EXPERIMENTS:
  1. Enriched embedding text (v6): summary_to_text() includes all fields
     - nDCG=0.493 (+17% vs v4), L1 ε=0.16, 249 tokens
     - WINNER — only approach that moved the needle
  2. Contrastive prompts (v7): more specific terms/entities in prompts
     - nDCG=0.484 (wash), L1 ε=0.14 (marginal), L2 ε=0.44 (worse)
  3. Content snippets (v8): first 200 chars of content in embeddings
     - nDCG=0.485 (wash), no improvement anywhere

  KEY FINDING: MiniLM 384-dim embedding space is the bottleneck, not summary
  text quality. All three approaches plateau at nDCG ~0.49. Adding more/better
  text doesn't help because the embedding model can't discriminate further.
  New pattern: "Embedding saturation."

  CURRENT STATE OF CODE:
  - cascade.py: cosine-only internal, unified summary_to_text()
  - builder.py: summary_to_text() with all fields + content_snippet
  - summarizer.py: contrastive prompts with specific term examples
  - hcr_baseline.py: beam=5, pfk=5, fk=3 (production defaults)
  - Cached tree: benchmark/results/hcr_tree.json (v8 tree with snippets)
  - Old trees backed up: hcr_tree_v6.json, hcr_tree_v7.json

  NEXT SESSION PRIORITY: Stronger embedding model.
  - Try all-mpnet-base-v2 (768-dim, drop-in replacement)
  - If that doesn't work, E5-large or BGE-large
  - Only need to re-embed summaries, no tree rebuild
  - Target: nDCG > 0.60, L1 ε ≤ 0.10

  Key files:
  - hcr_core/corpus/embedder.py — embedding model configuration
  - hcr_core/tree/builder.py — summary_to_text() and _embed_summary()
  - scripts/run_benchmark.py — re-embed step + benchmark runner

  Run: set -a && source .env && set +a && python scripts/run_benchmark.py --mode hcr
  Branch: 0215jc
