# Research State
# Machine-readable current state for session continuity
#
# Update this at session end and read at session start.

version: "1.0"
last_updated: "2026-02-13"
last_session: "2026-02-13"

# Current focus
current:
  phase: "implementation"  # discovery | research | implementation | validation
  focus: "RB-006 complete. All six research briefs done. Benchmark design is convergent (4/4 source agreement). Go/no-go decision: GO on Phase 1. Next: scaffold Phase 1 implementation starting with benchmark infrastructure."
  blockers: []

# Research briefs status
briefs:
  active: []
  completed:
    - id: "RB-001"
      title: "Prior art survey"
      completed: "2026-02-13"
      key_finding: "12+ systems in the space since 2024. Hierarchical retrieval outperforms flat RAG on complex tasks (2-20pp gains). LATTICE is closest to HCR. No system targets hard token budgets. RAPTOR collapsed-tree result challenges strict top-down traversal."
    - id: "RB-002"
      title: "Theoretical basis: elimination vs similarity"
      completed: "2026-02-13"
      key_finding: "Strict top-down elimination is theoretically fragile — error compounds as (1-ε)^d. RAPTOR collapsed-tree result is structurally predicted, not anomalous. Hybrid coarse-to-fine (shallow elimination + flat search within survivors) is theoretically optimal. H1 needs reframing from 'elimination > similarity' to 'hierarchical coarse-to-fine with token budgets > flat retrieval'."
    - id: "RB-003"
      title: "Scoring mechanics"
      completed: "2026-02-13"
      key_finding: "Cascade architecture (hybrid BM25+dense → cross-encoder) achieves ε ≈ 0.01–0.02 per level at ~40–80ms. Strict admissibility impossible for semantic relevance. Path-relevance EMA is higher leverage than per-node scoring. Summary quality is #1 upstream factor. AdaGReS submodular knapsack for token-budget selection. H1c → 75%."
    - id: "RB-004"
      title: "Tree construction"
      completed: "2026-02-13"
      key_finding: "Four-source convergence on construction recipe: top-down divisive clustering (bisecting k-means, d=2–3, b∈[6,15]) + LLM contrastive routing summaries + soft assignment (1–3 parents per leaf). Routing summaries are a distinct artifact class: {theme, includes, excludes, key_entities, key_terms}. Gemini adds Q-STRUM Debate for adversarial contrastive generation, depth-variable summary formatting (contrastive prose at L1, keyword arrays + multi-vector at L2), Schema Entropy Bound and Sibling Distinctiveness (BM25 softmax) as concrete tree quality metrics, and EraRAG LSH hyperplanes as alternative partitioning with deterministic incremental maintenance. Cross-branch defense requires five layers. No routing-specific tree quality metric in literature — genuine research gap. H1b → 80%."
    - id: "RB-005"
      title: "Failure modes"
      completed: "2026-02-13"
      key_finding: "No architectural showstopper. 26 failure modes identified across all pipeline stages. Overall expected failure rate: 10–20%. Top residual risks: (1) DPI information loss for detail queries, (2) budget impossibility for aggregation/listing queries, (3) beam collapse without diversity enforcement. Three design changes needed: beam diversity, collapsed-tree as co-primary, external source handling. Entity cross-links elevated to primary mechanism for dominant query type (entity-centric). Transition period (small corpus) is highest-risk deployment phase."
    - id: "RB-006"
      title: "Benchmark design"
      completed: "2026-02-13"
      key_finding: "Four-source convergence on benchmark design. Hybrid corpus (50K–100K chunks from GitLab handbook + EnronQA + synthetic injectors). 300–400 stratified queries with budget-feasibility labels. 7 core metrics led by per-level routing accuracy ε (never measured in any system) and answer sufficiency under token constraint. 4 baselines (BM25, hybrid flat, flat+CE as kill baseline, RAPTOR). Fail-fast sequence with kill criteria at each step. MVB costs $15–30 in LLM API calls. Success criteria: HCR@400 ≥ flat@400 + 5pp, ε ≤ 0.03 per level, dual-path ≥ single-path + 3pp. Kill: flat+CE beats HCR at full corpus with significance."

# Hypothesis summary
hypotheses:
  total: 3
  validated: 0
  invalidated: 0
  uncertain: 3
  retired: 1  # Original H1
  highest_confidence: "H1b (hybrid coarse-to-fine superiority) at 80%"
  lowest_confidence: "H1a (token efficiency) at 65%"
  notes: "All six research briefs complete. No showstoppers found. Architecture fully specified. Benchmark designed. Ready for empirical validation in Phase 1."

# Knowledge accumulation
knowledge:
  patterns_discovered: 0
  variants_discovered: 0
  contexts_documented: 0
  domains_documented: 0
  edge_cases: 0

# Next actions (prioritized)
next_actions:
  - "Go/no-go decision: all evidence supports GO"
  - "Update CLAUDE.md to reflect Phase 1 readiness"
  - "Scaffold Phase 1 project structure (benchmark as first deliverable)"
  - "Build corpus pipeline (GitLab handbook + EnronQA + synthetic injectors)"
  - "Implement flat+CE baseline first (it's the comparison target)"
  - "Build tree construction with instrumented ε logging"
  - "Run fail-fast experiments 1–3 before full evaluation"

# Notes for next session
notes: "RB-006 consolidation complete with 4/4 sources. Highest source agreement of any brief. Benchmark design is convergent — no fundamental conflicts, only calibration differences in thresholds. Key decisions: corpus at 50K–100K chunks (not docs), sufficiency@B as primary metric (not nDCG), ε₁ > 0.05 as kill threshold, contrastive-vs-generic summaries as highest-value component ablation. Phase 0 (Research & Validation) is complete. Phase 1 (Core Library) can begin. First implementation deliverable should be the benchmark infrastructure itself."
