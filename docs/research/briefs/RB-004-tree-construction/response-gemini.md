# RB-004: Tree Construction — Gemini Response

**Date:** 2026-02-13
**Model:** Gemini
**Prompt:** [RB-004 Prompt](./prompt.md)

---

*Paste response below this line.*
# **Tree Construction for Hierarchical Retrieval: Architectural Strategies and Optimization**

## **Introduction and Architectural Context**

The design of a tree construction strategy for a Hierarchical Context Retrieval system introduces fundamentally unique architectural constraints that distinguish it from conventional dense retrieval or standard flat-index Retrieval-Augmented Generation frameworks. In a system bounded by a strict operational token budget of fewer than four hundred tokens per routing decision, and mathematically constrained by per-level cascade scoring error rates, the structural integrity of the underlying retrieval index dictates the absolute upper limits of the system's accuracy. A cascade scoring architecture, which employs a hybrid sparse-dense first stage followed immediately by a computationally intensive cross-encoder reranking stage, is theoretically capable of driving per-level routing errors down to a marginal probability (![][image1]). However, as established by the foundational principles of probability and prior architectural reviews, the cumulative routing error across any top-down traversal path compounds geometrically according to the function ![][image2], where ![][image3] represents the per-level error rate and ![][image4] represents the depth of the hierarchical tree.

This exponential compounding penalty structurally invalidates deep, narrow tree topologies, establishing an absolute mathematical imperative for shallow, wide architectures characterized by depths of ![][image5] and branching factors of ![][image6]. However, conforming to these topological constraints introduces severe secondary challenges, primarily the Data Processing Inequality bottleneck. In information theory, the Data Processing Inequality dictates that post-processing cannot increase information content; in the context of hierarchical retrieval, this means that as information is propagated upward from granular leaf nodes to abstract root nodes, the summarization process acts as a highly lossy channel. Rare identifiers, specific nomenclature, numerical anchors, and precise structural markers—collectively termed "detail hooks"—are systematically washed out in favor of thematic abstractions. When a highly specific user query containing a rare entity is evaluated against a thematic parent summary, the cascade scorer lacks the requisite lexical and semantic signal to make an accurate routing decision, culminating in catastrophic traversal failure regardless of the cross-encoder's theoretical sophistication.

Furthermore, the foundational cluster hypothesis of information retrieval, which posits that documents relevant to the same query will inherently cluster together in vector space, has been empirically shown to degrade significantly when applied to broad, multi-topic corpora. While the hypothesis holds robustly for narrow, highly structured domains, failure rates can reach as high as forty-six percent in heterogeneous collections. When an organizational knowledge base spans diverse domains, rigid taxonomic clustering algorithms inevitably fracture multi-topic documents across disparate topological branches. Consequently, top-down routing algorithms are forced to abandon viable traversal paths prematurely, as the necessary evidence is distributed across branches that do not share an immediate parent node.

These intersecting constraints—the mathematical penalty of depth, the information loss of summarization, the failure of the cluster hypothesis on broad corpora, and the strict requirement that leaf nodes function as external source pointers rather than embedded content stores—necessitate a radical reevaluation of hierarchical tree construction. The ensuing report provides an exhaustive, rigorously detailed evaluation of partitioning algorithms, summary generation frameworks explicitly optimized for routing discrimination, mechanisms for managing cross-branch multi-topic content, dynamic maintenance strategies, and information-theoretic metrics for evaluating index quality independent of end-to-end retrieval benchmarks.

## **The Landscape of Tree Construction Methods**

The partitioning of a dynamic corpus into a hierarchical structure suitable for top-down cascade routing requires algorithms capable of balancing semantic cohesion with rigid topological constraints. The literature presents a highly divergent landscape of construction methodologies, each exhibiting distinct approaches to signal utilization, multi-topic handling, tree shape generation, and computational complexity.

### **Agglomerative Bottom-Up Clustering**

Hierarchical clustering has traditionally been dominated by bottom-up agglomerative approaches.1 Agglomerative methods initiate the clustering sequence by treating each individual document, or document chunk, as a singleton cluster. The algorithm iteratively computes a comprehensive similarity matrix between all pairs of clusters, subsequently merging the most proximate pairs based on predetermined linkage criteria—such as single linkage, average linkage, or Ward’s minimum variance method.1 Systems employing bottom-up strategies, such as the widely referenced RAPTOR framework, benefit significantly from their ability to capture highly granular, localized nested structures.4 Because the algorithm builds from the ground up, the immediate parent nodes maintain exceptionally high semantic fidelity to their leaf children.

However, agglomerative clustering presents severe architectural limitations for systems bounded by shallow depth requirements. First, the standard computational complexity of agglomerative clustering scales at ![][image7], or ![][image8] when heavily optimized with priority heap data structures.1 For large-scale organizational corpora ranging from tens of thousands to hundreds of thousands of chunks, this non-linear scaling renders the construction phase computationally prohibitive without the application of aggressive approximation techniques. Second, agglomerative methods inherently produce deep, highly unbalanced tree shapes. Without artificial algorithmic intervention, an agglomerative tree will frequently generate sprawling taxonomies on one branch while a sibling branch terminates almost immediately. This structural unpredictability directly violates the ![][image5] and ![][image6] parameters required for optimal cascade scoring. Finally, traditional agglomerative clustering enforces rigid, single-assignment hard boundaries, meaning a multi-topic document can only be merged into a single ascending path, structurally guaranteeing cross-branch retrieval failures for complex queries.

### **Divisive Top-Down Partitioning**

Conversely, divisive hierarchical clustering operates via a top-down paradigm, initiating the process with the entire corpus encapsulated within a single massive root cluster. The algorithm then recursively applies flat partitioning algorithms, most commonly bisecting k-means, to divide the vector space until a predefined stopping criterion or maximum depth is reached.1 Top-down methods are inherently greedy algorithms; they prioritize global partition boundaries over the preservation of local semantic neighborhoods.

For hierarchical context retrieval systems strictly bounded by shallow depth requirements, divisive methods offer compelling computational and topological advantages over agglomerative methods. Computationally, top-down approaches scale much more favorably, typically approaching ![][image9] complexity, as the recursion is terminated early to enforce the ![][image5] limit, entirely bypassing the exhaustive pairwise distance computations required at the granular leaf level.6 Furthermore, divisive algorithms can be explicitly parameterized to force balanced branching factors, seamlessly aligning with the ![][image6] requirement. Recent frameworks, including specific implementations evaluated within the LATTICE architecture, demonstrate that top-down divisive strategies explicitly force the model to identify broad, highly differentiating semantic hyperplanes at the root level before descending into granular sub-topics.6 This top-down forcing is structurally advantageous for routing, as the primary challenge in hierarchical retrieval is surviving the initial root-to-level-one routing decision.

### **Graph-Based Community Detection**

An alternative to vector-space Euclidean distance clustering is graph-based community detection, popularized by architectures such as GraphRAG and LightRAG. Rather than clustering dense embeddings, these systems first execute an exhaustive information extraction pass over the corpus, utilizing Large Language Models to extract entities, relationships, and attributes to construct a massive text-attributed bipartite graph.9 Once the graph is fully instantiated, community detection algorithms—such as Leiden or Louvain modularity optimization—are deployed to partition the graph into hierarchical semantic modules.12

Graph-based partitioning relies on topological connectivity and edge density rather than dense vector proximity. This provides a distinct, highly powerful advantage in handling multi-topic content. A document containing disparate themes or entities will naturally form semantic edges connecting to multiple distinct communities across the graph. This allows the community detection algorithm to capture highly nuanced, cross-cutting relationships that flat Euclidean distance metrics obscure.10 However, this architectural superiority comes at an astronomical computational cost. Extracting entities and constructing the initial graph requires excessive token consumption, with empirical reports indicating costs as high as $650 per one hundred queries for the initial LLM entity extraction phase alone. Furthermore, Leiden and Louvain algorithms inherently produce communities with highly variable, unpredictable sizing, leading to unbalanced branching factors that violate the strict ![][image6] cascade scoring constraint.

### **Spectral Clustering and Topic Models**

Spectral clustering introduces a mathematically rigorous approach to identifying non-linear cluster boundaries. By constructing an affinity matrix of the document embeddings and computing the eigenvectors of the corresponding graph Laplacian, spectral clustering projects the data into a lower-dimensional sub-manifold where traditional algorithms like k-means can easily separate the clusters. Spectral methods are highly effective at identifying interconnected conceptual manifolds that standard k-means would erroneously bifurcate. However, the requirement to perform an eigenvalue decomposition on an ![][image10] matrix introduces an ![][image7] computational bottleneck, requiring Nystrom approximations for corpora exceeding ten thousand chunks.

Concurrently, probabilistic topic models, such as Latent Dirichlet Allocation or modern transformer-based iterations like BERTopic, offer a unique partition signal based on word co-occurrence and latent topic distributions rather than pure semantic embedding distance. The defining advantage of topic models is their native support for soft clustering; documents are not assigned to a single cluster, but rather represented as a distribution across multiple topics. While mathematically elegant for addressing multi-topic documents, mapping a probabilistic mixed-membership model into a rigid hierarchical tree structure introduces significant routing ambiguities for deterministic cascade scorers.

### **Hyperplane-Based Locality Sensitive Hashing**

To bridge the gap between the computational speed of vector partitioning and the structural stability required for multi-topic handling and dynamic maintenance, emerging frameworks employ Locality Sensitive Hashing (LSH) for hierarchical tree construction. The EraRAG framework exemplifies this approach, utilizing random hyperplane projections to systematically segment the corpus.12

In the LSH paradigm, chunk embeddings are mathematically projected onto a set of randomly sampled, fixed hyperplanes. The algorithm calculates binary hash codes where each bit corresponds to the sign of the dot product between the embedding and a specific hyperplane.14 Embeddings demonstrating high cosine similarity will inevitably fall into the same Hamming space bucket. Because semantic distributions in real-world organizational corpora are highly skewed, initial LSH buckets are wildly uneven in size. To enforce strict branching factor requirements, the system applies a secondary deterministic merge-and-split operation. Small buckets that fail to meet a size threshold are agglomerated, while overly large buckets are recursively split using additional hyperplanes until the exact target node size is achieved.13 This LSH approach provides a highly controllable, perfectly deterministic topology that natively supports the ![][image5] depth constraint while operating at a fraction of the computational cost of agglomerative or graph-based methods.

Table 1 provides a comprehensive comparative analysis of these foundational tree construction methodologies, evaluated specifically against the constraints of hierarchical retrieval.

| Partitioning Method | Primary Signal | Tree Shape Output | Multi-Topic Handling | Computational Complexity | Maintenance Cost |
| :---- | :---- | :---- | :---- | :---- | :---- |
| **Agglomerative (Bottom-Up)** | Vector Distance Matrix | Deep, Unbalanced | Rigid / Single Assignment | ![][image8] | Extreme (Rebuild required) |
| **Divisive (Top-Down)** | Vector Centroids / Hyperplanes | Shallow, Balanced (Forced) | Rigid / Single Assignment | ![][image9] | High (Sub-tree rebuild) |
| **Community Detection (Graph)** | Entity-Edge Topology | Highly Unbalanced | Fluid (Multi-edge connectivity) | ![][image11] (Post-extraction) | Extreme (Unless upsert optimized) |
| **Spectral Clustering** | Eigenvectors of Laplacian | Balanced Sub-manifolds | Rigid / Single Assignment | ![][image7] (Standard) | Extreme (Matrix recomputation) |
| **Topic Models (BERTopic)** | Latent Co-occurrence | Flat / Shallow | Native Soft-Clustering | ![][image12] (Inference phase) | Moderate (Distribution shift) |
| **Hyperplane LSH (EraRAG)** | Random Projection Dots | Controlled via Merge/Split | Deterministic One-to-One | ![][image13] (h \= hyperplanes) | Low (Localized routing) |

## **Summary Generation Optimized for Routing Reliability**

The highest-leverage architectural intervention in hierarchical context retrieval is the optimization of the intermediate node summaries. When intermediate nodes are represented by generic, human-readable thematic prose, the Data Processing Inequality dictates that the specific nomenclature required to route granular, detail-oriented queries is irrevocably lost. A node summary in this architecture must not be designed for human reading comprehension; it must be explicitly engineered as a discriminatory, machine-readable routing apparatus. No amount of downstream cross-encoder sophistication can compensate for a summary that has structurally erased the entities required for a routing match.

### **Contrastive Summarization and Debate Prompting**

If a cascade scorer is to accurately evaluate whether a target leaf document resides within a specific branch, the summary of that branch must clearly delineate its semantic boundaries against its immediate siblings. Generic summarization methodologies inherently fail this requirement by focusing entirely on what a branch contains, rather than defining what it explicitly excludes.

Contrastive summarization directly targets this vulnerability. Frameworks such as PRISM and early iterations of STRUM-LLM attempt to extract features that differentiate clusters, but these models proved insufficient for generating the highly discriminatory signals required for deterministic routing.15 The advancement of query-driven contrastive summarization, specifically exemplified by the Q-STRUM Debate methodology, introduces a rigorous, multi-stage argumentation process to mathematically synthesize structural boundaries.16

In the Q-STRUM Debate framework, the system utilizes Large Language Models to simulate a structured adversarial interaction between programmatic personas prior to generating the summary. One persona is instructed to argue aggressively for the relevance of a specific semantic cluster, emphasizing its exact contents, extracting verbatim detail hooks, and simultaneously highlighting the perceived deficiencies or omissions of the sibling cluster. The opposing persona performs the precise inverse operation.16 Following this simulated argumentation phase, a synthesis layer strips the persona framing and distills the output into an explicitly contrastive structure.16 The resulting summary explicitly states hard boundary conditions—for instance, "This branch contains deployment metrics and uptime statistics for the cloud database migration, but explicitly does NOT contain the rollback procedures, compliance auditing, or financial cost analysis, which are located in the sibling branches." This structural negation provides the cross-encoder with definitive, irrefutable negative signals, radically reducing the probability of false-positive routing decisions at the root level.

### **Format Structure and Information Density**

Within the strict constraint of a 400-token budget, the format of the summary dictates its routing utility. Empirical evidence strongly suggests that pure narrative prose is highly inefficient for routing. Natural language generation introduces syntactic filler, transitionary phrasing, and redundant semantic tokens that consume budget without providing discriminatory signal.

Instead, node summaries must transition to highly structured formats. A routing-optimized summary format typically consists of three distinct layers. The first layer is a high-density keyword and exact-entity array, essentially a comma-separated list of the rarest, most statistically significant identifiers (e.g., specific API endpoint names, specific project codenames) contained within the descendant leaves. The second layer is a concise contrastive boundary statement, derived from the Q-STRUM methodology, explicitly outlining the cluster's exclusion criteria. The final layer is a brief narrative synthesis to provide the cross-encoder with the contextual framing required to resolve syntactic ambiguities. This structured hierarchy ensures that the cascade scorer evaluates the query against a dense matrix of detail hooks before falling back on general semantic similarity.

### **Multi-Vector Representations for Detail Preservation**

Even with highly structured, contrastive prose, the standard industry practice of embedding an entire node summary into a single dense vector creates a severe representational bottleneck. A single 768-dimensional or 1024-dimensional vector forces disparate, orthogonal concepts into a conflicted, geometrically averaged representation.18 When a highly specific query containing a rare detail hook is processed by the cascade scorer, its embedding will fail to achieve high cosine similarity with an averaged parent vector, as the signal of the rare entity has been diluted by the dominant thematic tokens.

To circumvent this mathematical limitation, hierarchical architectures are increasingly adopting late-interaction, multi-vector representations at the summary level, following the ColBERT (Contextualized Late Interaction over BERT) paradigm.18 Instead of compressing a 300-token contrastive summary into a single dense vector, a ColBERT-style multi-vector model generates and stores a separate embedding for every individual token within the summary.18 During the routing phase, the MaxSim operator evaluates the query tokens against the massive matrix of summary tokens. This allows specific query terms to bind tightly to specific token representations in the summary without any interference from unrelated concepts.19 Consequently, detail hooks are preserved with absolute geometric fidelity.

While maintaining hundreds of vectors per internal node substantially increases the memory footprint and operational latency of the index, recent advancements in token pooling and hierarchical quantization demonstrate that the storage cost of multi-vector indexes can be aggressively pruned.19 Clustering-based token pruning algorithms can reduce the number of required vectors by fifty to seventy-five percent with virtually zero measurable degradation in retrieval accuracy.22 By representing a node not as a prose paragraph, but as a heavily pruned, multi-vector matrix of highly specific detail-hook embeddings, the cascade scorer retains the granular mathematical signal necessary to safely navigate the upper echelons of the tree.

Table 2 contrasts various summary representation topologies and their empirical routing efficacies.

| Summary Representation | Data Processing Inequality Impact | Boundary Definition | Preserves Detail Hooks | Retrieval Latency |
| :---- | :---- | :---- | :---- | :---- |
| **Standard Dense Vector** | Severe (Averaging effect) | Poor (Thematic only) | No (Washed out geometrically) | Lowest (Max Inner Product) |
| **Contrastive Prose (Dense)** | Moderate | High (Explicit negation) | Partial (Depends on extraction) | Low |
| **Multi-Vector (ColBERT)** | Minimal | Moderate | Yes (Absolute token-level mapping) | Higher (MaxSim operations) |
| **Pruned Multi-Vector \+ Contrastive** | Minimal | High | Yes (Retained via algorithmic pooling) | Moderate |

## **Structural Management of Multi-Topic Content**

The most prominent architectural failure mode in hierarchical retrieval, explicitly identified in early literature reviews, occurs when highly relevant content naturally spans multiple semantic branches. When organizational documents contain cross-cutting themes—such as a legal document analyzing the engineering parameters of a new product—standard partitioning algorithms force a hard, binary assignment to a single branch. A top-down query traversing the hierarchy may be perfectly matched semantically to the unselected branch, resulting in a total retrieval failure because the necessary evidence is permanently shielded within an inaccessible subtree.13

Solving this cross-branch routing failure requires moving beyond naive hard clustering. The literature presents several divergent strategies: soft clustering via duplication, non-tree topological structures, query-time multi-path expansion, and deterministic content decomposition.

### **Soft Clustering and Node Duplication**

Soft clustering, theoretically supported by Gaussian Mixture Models or probabilistic topic models, permits a single document or chunk to exist in multiple leaf nodes simultaneously. If a document exhibits a thirty percent probability of belonging to an engineering branch and a seventy percent probability of belonging to a legal branch, soft clustering simply places a pointer in both subtrees.

While this mathematically eliminates the risk of cross-branch query failures, it introduces cascading architectural inefficiencies. Duplicated content artificially inflates the token volume of summary nodes at every level above the duplication, heavily diluting the information density of the summaries. Furthermore, in an agentic system where dynamic maintenance is paramount, updating or deprecating a single document requires the system to track and independently execute updates across multiple disconnected subtrees, creating severe consistency hazards and dramatically escalating write amplification.

### **Directed Acyclic Graphs and Link Structures**

A more sophisticated approach involves relaxing the strict single-parent constraint of traditional trees, transforming the index into a Directed Acyclic Graph (DAG). In a DAG structure, a multi-topic leaf node, or even an entire sub-branch, can be connected to multiple parent nodes across different domains. This allows the internal routing summaries to remain distinct while sharing access to cross-cutting leaves.

However, implementing a DAG introduces severe complications for cascade scoring algorithms. Top-down scorers inherently assume that path probabilities are independent. In a DAG, paths recombine, meaning the scorer may waste compute cycles evaluating the same downstream node from multiple angles, or conversely, artificially inflate the relevance score of a node due to multi-path convergence.

### **Content Decomposition and Query-Time Expansion**

Frameworks optimized for extreme scalability and dynamic updating eschew soft clustering and DAGs entirely, enforcing strict one-to-one assignments but managing multi-topic complexity through granular pre-clustering decomposition.12 Rather than treating a massive, multi-topic document as a monolithic entity, the system utilizes LLMs to chunk the document into highly granular, semantically homogeneous segments prior to index construction. Using Locality Sensitive Hashing or divisive clustering, these localized, single-topic chunks are deterministically assigned to unique, disparate buckets.14

If a cluster becomes overly heterogeneous during this process—indicating that a multi-topic chunk was improperly grouped—the system intervenes with a merge-and-split strategy.13 By mathematically projecting the problematic bucket across additional hyperplanes, the system shatters the heterogeneous cluster into smaller, highly coherent segments.12 This approach guarantees that complex documents are structurally decomposed so that their constituent themes reside in appropriate, distinct branches.

During the actual retrieval phase, cross-branch queries are natively resolved not by finding one duplicated document, but through query-time multi-path expansion. The cascade scorer is permitted to explore the top ![][image14] branches at any level (e.g., beam search). When a user queries the legal engineering parameters, the scorer independently routes down both the legal branch and the engineering branch, successfully retrieving the decomposed single-topic chunks and allowing the LLM to synthesize the final answer. This preserves structural purity while maximizing recall.

## **Topological Optimization Under Shallow-Wide Constraints**

The mathematical necessity of the ![][image5] and ![][image6] topology fundamentally alters tree construction priorities. In deep networks, routing algorithms have multiple hierarchical layers to dynamically correct minor directional ambiguities or recover from suboptimal branching decisions. In a shallow network, every single node evaluation is a high-stakes decision; a single routing error at the root level instantly terminates the retrieval process. Therefore, the topology itself must be aggressively optimized to maximize information density and routing clarity.

### **Enforcing Balanced Architectures**

Standard tree generation algorithms, particularly bottom-up agglomerative methods, produce highly unbalanced topologies. It is common to see trees where one branch contains a massive, deeply nested taxonomy of fifty nodes while a sibling branch terminates immediately at a single leaf. For a routing-optimized index operating under a strict token budget, unbalanced trees are structurally detrimental. An internal node forced to summarize fifty children requires a summary matrix far too complex and broad to fit within a 400-token limit, leading to catastrophic abstraction. Conversely, a node with only two children wastes valuable parallel scoring capacity that the hardware could otherwise utilize.

To strictly enforce the ![][image6] constraint, divisive top-down clustering must be heavily parameterized. Algorithms like bisecting k-means can be explicitly modified to enforce strict minimum and maximum cluster size bounds, halting the splitting operation only when the exact required branching factor is achieved uniformly at each level. Alternatively, the hyperplane LSH approach provides an elegant mathematical mechanism for topological control. By dynamically tuning the LSH bucket size thresholds, the algorithm continuously triggers iterative merge-and-split operations until the graph achieves perfect balanced granularity across all horizontal planes, ensuring that no single node carries an disproportionate summarization burden.13

### **Information Density and Variable Granularity**

Within the strict depth limits of ![][image5], the allocation of information density across the vertical axis becomes a critical optimization problem. The root node must serve as a universal dispatcher, requiring the summaries of its immediate children (Level 1\) to be maximally divergent. As the traversal descends to ![][image15] and ![][image16], the routing requirements change; the summaries must shift from divergent, exclusionary boundary definitions to highly specific, inclusionary entity enumerations.

Optimal topologies do not distribute semantic granularity uniformly. Instead, they employ heterogeneous summary formatting across depths. Level 1 summaries prioritize the contrastive debate-style prose discussed previously, establishing strict, non-overlapping categorical boundaries to ensure the initial routing decision is flawless. Level 2 summaries, however, abandon prose almost entirely, relying instead on high-density keyword arrays and pruned multi-vector ColBERT embeddings to serve as explicit detail hooks for the final leaf-pointer selection. This variable formatting maximizes the utility of the strict token budget, allocating narrative tokens where disambiguation is required and dense entity vectors where exact matching is strictly necessary.

Table 3 models the optimal information distribution strategy across a constrained ![][image16] topology.

| Tree Level | Routing Objective | Optimal Summary Format | Primary Evaluation Metric |
| :---- | :---- | :---- | :---- |
| **Level 0 (Root)** | Initial query evaluation | N/A (Entry point) | N/A |
| **Level 1 (Branches)** | Macro-disambiguation | Contrastive Prose \+ Exclusions | Sibling Distinctiveness (SD) |
| **Level 2 (Twigs)** | Entity matching | Keyword Arrays \+ Multi-Vector | Term Coherency (TC) |
| **Level 3 (Leaves)** | Final context fetching | External Source Pointers | External API Latency |

## **Dynamic Index Maintenance and Incremental Evolution**

The assumption of a static, immutable corpus is a fatal flaw for production retrieval systems. As an agentic organizational knowledge base evolves—ingesting daily reports, modifying codebases, and deprecating outdated policies—the underlying semantic distribution shifts continuously. Traditional clustering architectures suffer from a catastrophic inability to accommodate incremental insertions gracefully.

### **The Global Reconstruction Bottleneck**

Architectures entirely reliant on global clustering algorithms, such as standard k-means, agglomerative linkage, or community detection frameworks like the original GraphRAG implementation, fundamentally lack mechanisms for localized structural updates.23 When a new document is introduced to the system, these frameworks must essentially dismantle their existing cluster centroids or hierarchical community structures and recompute the entire index from scratch.23

This constraint makes such frameworks practically unusable for continuously growing corpora. Rebuild times scale linearly or exponentially, completely locking the index during the update window. More critically, the financial and computational token consumption associated with utilizing an LLM to regenerate all hierarchical summaries from the ground up becomes exponentially unviable.23 For a system managing tens of thousands of chunks, a minor daily update cannot necessitate a full structural rebuild.

### **Localized Hyperplane Updates**

To achieve dynamic, low-latency maintenance, the system must support mathematical determinism and strictly localized propagation. The EraRAG architecture elegantly solves this through the continuous preservation of its LSH hyperplanes.12 Because the hyperplanes used to initially partition the vector space are saved deterministically, any new document added to the system can be mathematically projected through the exact same hyperplanes. This guarantees that the new embedding falls instantly into the correct existing semantic bucket without requiring any global distance recomputations or distance matrix updates.12

Once the document is routed to its target bucket, the update process is strictly localized. The system re-summarizes only the immediately affected leaf bucket. If the new insertion causes the bucket to exceed its predefined branching size threshold, a split operation is triggered locally. This structural modification then propagates upward algorithmically; the parent node re-summarizes to reflect the newly split children, but the sibling branches across the tree remain entirely untouched.13 This localized upward propagation reduces graph update times by an order of magnitude and drastically curtails token consumption, allowing the hierarchical tree to evolve dynamically in real-time.13

### **Incremental Upsert and Lazy Rebalancing**

Similarly, frameworks like LightRAG and VersionRAG employ incremental upsert algorithms that seamlessly integrate new nodes into existing structures without triggering global rebuilds.10 In these systems, incoming documents are processed and directly attached to the nearest conceptual neighbors based on an approximate nearest neighbor search.

To prevent the tree from becoming unbalanced over time due to localized heavy insertions, the system employs a background lazy-evaluation process. A lightweight daemon continuously monitors node density and Sibling Distinctiveness scores. If a specific conceptual branch becomes bloated or mathematically indistinct, an asynchronous background job is triggered to re-cluster only that specific sub-branch, optimizing its internal summaries and rebalancing its topology while the rest of the retrieval system remains fully online and operational.28

Table 4 outlines the operational trade-offs of dynamic maintenance strategies.

| Maintenance Strategy | Computational Trigger | Scope of Recomputation | Determinism | Applicability to HCR |
| :---- | :---- | :---- | :---- | :---- |
| **Global Reconstruction** | Scheduled interval | Entire corpus | Low (New centroids generated) | Unviable (High token cost) |
| **Incremental Upsert (Lazy)** | Node density threshold | Single sub-branch | Moderate | High |
| **LSH Hyperplane Routing** | Immediate upon insertion | Localized upward path | Absolute | Optimal (Scale invariant) |

## **The Role of Large Language Models in Tree Construction**

While embedding models are highly efficient at determining semantic distance, the integration of generative Large Language Models into the construction pipeline yields substantial, though expensive, structural benefits. Determining exactly where to deploy LLMs in the pipeline is critical for optimizing the cost-to-quality ratio.

In early architectures like RAPTOR, the LLM is deployed exclusively as a summarization engine, operating after standard GMM clustering has defined the boundaries. While this improves the readability of the nodes, it does not correct fundamental partitioning errors; if the embedding model groups orthogonal concepts, the LLM simply generates a confused summary. In extreme architectures like GraphRAG, the LLM is used at the very beginning of the pipeline to extract entities and build relationships, completely bypassing embedding limitations. However, at $650 per 100 queries for extraction, this approach is financially unsustainable for large-scale dynamic maintenance.

The optimal integration point for LLMs is demonstrated in architectures like LATTICE, where the LLM serves as a semantic routing and partitioning judge.7 Rather than blindly trusting vector distances, the LLM is used to evaluate boundary decisions during the divisive top-down clustering phase. By explicitly prompting the LLM to assess whether a proposed cluster split successfully separates distinct themes, the system ensures that the hierarchical boundaries map to human-logical concepts rather than arbitrary geometric vector spaces.7 Furthermore, utilizing LLMs specifically for the Q-STRUM contrastive debate summarization—restricting heavy generation only to the top two levels of the tree—maximizes routing accuracy while strictly capping token expenditure.

## **Evaluating Tree Quality Independently of Retrieval**

Relying exclusively on end-to-end retrieval benchmarks (e.g., Recall@100 on downstream tasks) to evaluate tree construction introduces massive confounding variables. If a query fails, it is impossible to determine whether the failure stemmed from a poorly tuned cross-encoder, suboptimal prompt design, or a structurally deficient tree. Evaluating the structural integrity of the index requires mathematically rigorous, routing-specific metrics. Generic clustering metrics, such as Silhouette scores or the Davies-Bouldin index, measure spatial density in Euclidean vector space, which has near-zero predictive validity for top-down routing algorithms traversing complex, multi-topic semantic boundaries.

### **Sibling Distinctiveness and Term Coherency**

A structurally sound tree designed for hierarchical cascade retrieval must absolutely maximize the semantic differentiation between nodes sharing a common parent. The Sibling Distinctiveness (SD) metric, initially introduced in advanced taxonomy generation frameworks like CoRel, explicitly quantifies how thoroughly a specific branch is separated from its immediate topological neighbors.29

While early academic implementations relied on manual human evaluation to score SD 31, modern automated variants compute SD using precise term-frequency distributions. For any class-indicative term extracted within a branch, the automated SD formula applies a softmax function over the BM25 relevance scores of that specific term across all sibling branches.33 If a highly descriptive detail hook appears frequently in the target branch but has a near-zero probabilistic occurrence in all sibling branches, the SD score approaches 1.0. A tree characterized by a high average Sibling Distinctiveness score is mathematically guaranteed to provide clear, unambiguous signals to the cascade scorer, neutralizing the risk of arbitrary routing.

Coupled inextricably with SD is the metric of Term Coherency (TC), which measures the internal semantic density and mutual information of the words within a single node.30 A node exhibiting high TC and high SD represents an optimal partition: it is internally unified and externally isolated.

### **Information-Theoretic Evaluation**

Moving beyond lexical distribution, tree quality can be rigorously evaluated through the lens of pure information theory. Advanced frameworks such as U-AHC and schema evaluation models conceptualize the entire hierarchical retrieval system not as a database, but as a cascade of information channels.34

The structural quality of the index explicitly determines the bandwidth capacity of these communication channels. According to the Schema Entropy Bound theorem, the maximum retrieval capacity of a system is strictly bounded by the minimum of the embedding space dimensionality and the Shannon entropy of the database schema.34 The Shannon entropy is calculated as ![][image17], where ![][image18] represents the probability distribution of semantic concepts within the tree.34

If a tree is constructed poorly—characterized by overly broad, overlapping thematic summaries—the probability distribution ![][image18] becomes highly uniform across the nodes. A uniform distribution maximizes uncertainty and severely degrades the mathematical channel capacity. Conversely, a highly optimized tree concentrates specific concepts into discrete, heavily guarded nodes, creating a sharp, highly skewed probability distribution that maximizes the mutual information between the node summary and its actual descendant content. By analyzing the mutual information transfer at each specific depth level, system architects can pinpoint exactly which horizontal layer of the tree is failing to transmit sufficient routing signal to the cascade scorer.

Table 5 compares the theoretical applicability and predictive validity of structural evaluation metrics.

| Evaluation Metric | Mathematical Basis | Measures | Predictive Validity for Routing |
| :---- | :---- | :---- | :---- |
| **Silhouette Score** | Euclidean intra/inter ratio | Spatial clustering density | Low (Ignores semantic boundaries) |
| **Sibling Distinctiveness** | BM25 Softmax over siblings | Node boundary clarity | Very High |
| **Term Coherency** | Mutual info of node terms | Internal semantic unity | High |
| **Schema Entropy Bound** | Shannon Entropy ![][image19] | Information channel capacity | Very High (Detects DPI loss) |

## **Integration Constraints: External Source Pointers as Leaves**

A defining constraint of the target system is that leaf nodes are not embedded, static text chunks stored locally, but rather external source pointers mapping to disparate enterprise APIs, organizational databases, or dynamic document repositories. This architecture brilliantly decouples the index from the underlying storage, drastically reducing the system footprint and preventing data duplication, but it fundamentally alters the entire tree construction pipeline. Because the full text of leaf documents may be highly volatile, computationally expensive to fetch, or even completely inaccessible in its entirety during offline construction, the hierarchical tree must often be synthesized exclusively from partial data and metadata representations.

### **Metadata-Augmented Indexing**

Constructing a robust semantic hierarchy without full-text access requires specialized, decoupled indexing protocols. Emerging frameworks such as CHORUS demonstrate that highly accurate hierarchical trees can be successfully generated using metadata-augmented representations.36 In enterprise environments where the underlying data consists of complex codebases, external quantitative datasets, or volatile API endpoints, relying purely on raw content extraction creates severe vocabulary mismatches between natural language user queries and the underlying syntax.37

To build the tree without full text, the system must ingest vast arrays of descriptive metadata—including structural tags, author profiles, system timestamps, organizational categorical hierarchies, and execution summaries—and synthesize these disparate data points into cohesive surrogate text representations.37 The top-down divisive clustering algorithms or the LSH hyperplanes subsequently operate exclusively on these metadata-derived surrogates rather than the inaccessible full text. This ensures that the tree topology maps accurately to the semantic intent of the data rather than its volatile raw string representation.

### **Federated Synthesis and Drift Detection**

When leaves function as pointers to external sources that mutate independently of the retrieval index, the carefully crafted node summaries face the constant threat of rapid semantic obsolescence. If an external engineering API fundamentally changes its payload structure, or a linked policy document is entirely rewritten by an external department, the parent summary residing in the retrieval tree effectively becomes a dead link, directing the cascade scorer toward endpoints that no longer contain the promised context.

To permanently mitigate this risk, the tree construction process must incorporate dynamic validation loops directly analogous to historical federated search architectures. During the lazy-rebalancing phase described in the dynamic maintenance section, the system must continually execute lightweight, asynchronous polling of the external leaf pointers. By fetching a small statistical sample of the external data at regular intervals, the system continuously calculates the semantic drift between the newly fetched sample and the original metadata surrogate used during initial tree construction.

If this calculated drift exceeds a predefined Kullback-Leibler divergence threshold, the specific leaf surrogate is instantly invalidated. This invalidation immediately triggers a localized LSH upward-propagating update to rewrite the parent summaries, ensuring they perfectly reflect the new external reality.13 This continuous federated synthesis ensures that the hierarchical index remains a highly accurate, flawlessly synchronized routing map to external data sources without ever requiring the constant, full-scale ingestion of the underlying enterprise repositories.

## **Conclusion**

Constructing a hierarchical tree optimized specifically for cascade-scored routing under hard token budgets demands a fundamental departure from traditional clustering methodologies. The rigid structural limits imposed by the ![][image20] error cascade and the lossy nature of the Data Processing Inequality dictate that generic, deep taxonomic trees will mathematically cap retrieval accuracy, rendering downstream cross-encoder sophistication irrelevant.

To maximize routing efficacy and eliminate cross-branch failures, the architecture must transition toward top-down, structurally forced ![][image5] topologies. Utilizing Locality Sensitive Hashing hyperplanes ensures absolute determinism and facilitates localized, highly scalable dynamic maintenance without triggering catastrophic global rebuilds. Furthermore, multi-topic content must be strictly segmented via content decomposition rather than soft-clustered, utilizing merge-and-split boundary enforcement to isolate overlapping themes into discrete, accessible paths.

Crucially, the internal nodes of the tree cannot rely on standard extractive summarization. They must be engineered as explicitly contrastive routing apparatuses, leveraging simulated adversarial debate prompting to generate clear, irrefutable boundary delineations. To preserve the detail hooks necessary for granular query routing, these summaries must be represented through highly pruned multi-vector late-interaction matrices rather than standard single dense embeddings. Finally, the structural integrity of this architecture must be continuously validated using routing-specific metrics—specifically automated Sibling Distinctiveness and Schema Entropy Bound evaluations—to ensure that every node maintains the requisite channel capacity to guide the cascade scorer flawlessly toward the dynamic external leaf pointers. Implementing these intersecting optimizations will yield an index structure capable of bypassing the historical limitations of the cluster hypothesis, delivering maximum fidelity routing across complex, evolving enterprise environments.

#### **Works cited**

1. What is Hierarchical Clustering? \- IBM, accessed on February 13, 2026, [https://www.ibm.com/think/topics/hierarchical-clustering](https://www.ibm.com/think/topics/hierarchical-clustering)  
2. Comparative Study of Hierarchical Clustering over Partitioning Clustering Algorithm \- SMEC, accessed on February 13, 2026, [https://www.smec.ac.in/assets/images/committee/research/17-18/504.Comparative%20Study%20of%20Hierarchical%20Clustering%20over%20Partitioning%20Clustering%20Algorithm.pdf](https://www.smec.ac.in/assets/images/committee/research/17-18/504.Comparative%20Study%20of%20Hierarchical%20Clustering%20over%20Partitioning%20Clustering%20Algorithm.pdf)  
3. A Comparison of Document Clustering Techniques \- Statistics & Data Science, accessed on February 13, 2026, [https://www.stat.cmu.edu/\~rnugent/PCMI2016/papers/DocClusterComparison.pdf](https://www.stat.cmu.edu/~rnugent/PCMI2016/papers/DocClusterComparison.pdf)  
4. Hierarchical Clustering: A Tree-Based Approach to Data Grouping | by Abhay singh, accessed on February 13, 2026, [https://medium.com/@abhaysingh71711/hierarchical-clustering-a-tree-based-approach-to-data-grouping-241131b1c4c5](https://medium.com/@abhaysingh71711/hierarchical-clustering-a-tree-based-approach-to-data-grouping-241131b1c4c5)  
5. Hierarchical Clustering in Machine Learning \- GeeksforGeeks, accessed on February 13, 2026, [https://www.geeksforgeeks.org/machine-learning/hierarchical-clustering/](https://www.geeksforgeeks.org/machine-learning/hierarchical-clustering/)  
6. Paper page \- LLM-guided Hierarchical Retrieval \- Hugging Face, accessed on February 13, 2026, [https://huggingface.co/papers/2510.13217](https://huggingface.co/papers/2510.13217)  
7. arxiv.org, accessed on February 13, 2026, [https://arxiv.org/html/2510.13217v1](https://arxiv.org/html/2510.13217v1)  
8. \[2510.13217\] LLM-guided Hierarchical Retrieval \- arXiv, accessed on February 13, 2026, [https://arxiv.org/abs/2510.13217](https://arxiv.org/abs/2510.13217)  
9. GraphRAG: Practical Guide to Supercharge RAG with Knowledge Graphs \- LearnOpenCV, accessed on February 13, 2026, [https://learnopencv.com/graphrag-explained-knowledge-graphs-medical/](https://learnopencv.com/graphrag-explained-knowledge-graphs-medical/)  
10. LightRAG, accessed on February 13, 2026, [https://lightrag.github.io/](https://lightrag.github.io/)  
11. EraRAG: Efficient and Incremental Retrieval Augmented Generation for Growing Corpora \- arXiv, accessed on February 13, 2026, [https://arxiv.org/pdf/2506.20963](https://arxiv.org/pdf/2506.20963)  
12. EraRAG: Efficient and Incremental Retrieval Augmented Generation for Growing Corpora, accessed on February 13, 2026, [https://arxiv.org/html/2506.20963v2](https://arxiv.org/html/2506.20963v2)  
13. EraRAG: A Scalable, Multi-Layered Graph-Based Retrieval System for Dynamic and Growing Corpora \- MarkTechPost, accessed on February 13, 2026, [https://www.marktechpost.com/2025/07/25/erarag-a-scalable-multi-layered-graph-based-retrieval-system-for-dynamic-and-growing-corpora/](https://www.marktechpost.com/2025/07/25/erarag-a-scalable-multi-layered-graph-based-retrieval-system-for-dynamic-and-growing-corpora/)  
14. EraRAG: Efficient and Incremental Retrieval Augmented ... \- arXiv, accessed on February 13, 2026, [https://arxiv.org/abs/2506.20963](https://arxiv.org/abs/2506.20963)  
15. PRISM: Learning Design Knowledge from Data for Stylistic Design Improvement \- arXiv, accessed on February 13, 2026, [https://arxiv.org/html/2601.11747v1](https://arxiv.org/html/2601.11747v1)  
16. Q-STRUM Debate: Query-Driven Contrastive ... \- ACL Anthology, accessed on February 13, 2026, [https://aclanthology.org/2025.findings-acl.1170.pdf](https://aclanthology.org/2025.findings-acl.1170.pdf)  
17. Q-STRUM Debate: Query-Driven Contrastive Summarization for Recommendation Comparison \- arXiv, accessed on February 13, 2026, [https://arxiv.org/html/2502.12921v1](https://arxiv.org/html/2502.12921v1)  
18. A little pooling goes a long way for multi-vector representations \- Answer.AI, accessed on February 13, 2026, [https://www.answer.ai/posts/colbert-pooling.html](https://www.answer.ai/posts/colbert-pooling.html)  
19. P4: Late Interaction Models For RAG \- Hamel Husain, accessed on February 13, 2026, [https://hamel.dev/notes/llm/rag/p4\_late\_interaction.html](https://hamel.dev/notes/llm/rag/p4_late_interaction.html)  
20. Bridging Language Gaps: Advances in Cross-Lingual Information Retrieval with Multilingual LLMs \- arXiv, accessed on February 13, 2026, [https://arxiv.org/html/2510.00908v1](https://arxiv.org/html/2510.00908v1)  
21. ESPN: Memory-Efficient Multi-Vector Information Retrieval \- arXiv, accessed on February 13, 2026, [https://arxiv.org/html/2312.05417v1](https://arxiv.org/html/2312.05417v1)  
22. Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling \- arXiv, accessed on February 13, 2026, [https://arxiv.org/html/2409.14683v1](https://arxiv.org/html/2409.14683v1)  
23. The Hidden Cost of 98% Accuracy: A Practical Guide to RAG Architecture Selection \- Tao An, accessed on February 13, 2026, [https://tao-hpu.medium.com/the-hidden-cost-of-98-accuracy-a-practical-guide-to-rag-architecture-selection-6883adc5289c](https://tao-hpu.medium.com/the-hidden-cost-of-98-accuracy-a-practical-guide-to-rag-architecture-selection-6883adc5289c)  
24. Retrieval-Augmented Generation with Knowledge Graphs: A Survey \- OpenReview, accessed on February 13, 2026, [https://openreview.net/pdf?id=ZikTuGY28C](https://openreview.net/pdf?id=ZikTuGY28C)  
25. EraRAG: Efficient and Incremental Retrieval Augmented Generation for Growing Corpora | Request PDF \- ResearchGate, accessed on February 13, 2026, [https://www.researchgate.net/publication/393065801\_EraRAG\_Efficient\_and\_Incremental\_Retrieval\_Augmented\_Generation\_for\_Growing\_Corpora](https://www.researchgate.net/publication/393065801_EraRAG_Efficient_and_Incremental_Retrieval_Augmented_Generation_for_Growing_Corpora)  
26. VersionRAG: Version-Aware Retrieval-Augmented Generation for Evolving Documents, accessed on February 13, 2026, [https://arxiv.org/html/2510.08109v1](https://arxiv.org/html/2510.08109v1)  
27. LightRAG: Graph-Enhanced Text Indexing and Dual-Level Retrieval, accessed on February 13, 2026, [https://promptengineering.org/lightrag-graph-enhanced-text-indexing-and-dual-level-retrieval/](https://promptengineering.org/lightrag-graph-enhanced-text-indexing-and-dual-level-retrieval/)  
28. LightRAG/README.md at main \- GitHub, accessed on February 13, 2026, [https://github.com/HKUDS/LightRAG/blob/main/README.md](https://github.com/HKUDS/LightRAG/blob/main/README.md)  
29. Automatically Discovering How Misogyny is Framed on Social Media \- ACL Anthology, accessed on February 13, 2026, [https://aclanthology.org/2025.naacl-long.608.pdf](https://aclanthology.org/2025.naacl-long.608.pdf)  
30. \[Quick Review\] CoRel: Seed-Guided Topical Taxonomy Construction by Concept Learning and Relation Transferring \- Liner, accessed on February 13, 2026, [https://liner.com/review/corel-seedguided-topical-taxonomy-construction-by-concept-learning-and-relation](https://liner.com/review/corel-seedguided-topical-taxonomy-construction-by-concept-learning-and-relation)  
31. RaTE: a Reproducible automatic Taxonomy ... \- ACL Anthology, accessed on February 13, 2026, [https://aclanthology.org/2023.iwcs-1.18.pdf](https://aclanthology.org/2023.iwcs-1.18.pdf)  
32. CoRel: Seed-Guided Topical Taxonomy Construction by Concept Learning and Relation Transferring \- arXiv, accessed on February 13, 2026, [https://arxiv.org/pdf/2010.06714](https://arxiv.org/pdf/2010.06714)  
33. TELEClass: Taxonomy Enrichment and LLM-Enhanced Hierarchical Text Classification with Minimal Supervision \- arXiv, accessed on February 13, 2026, [https://arxiv.org/html/2403.00165v3](https://arxiv.org/html/2403.00165v3)  
34. An Information-Theoretic Framework for Retrieval-Augmented Generation Systems \- MDPI, accessed on February 13, 2026, [https://www.mdpi.com/2079-9292/14/15/2925](https://www.mdpi.com/2079-9292/14/15/2925)  
35. An Information-Theoretic Approach to Hierarchical Clustering of Uncertain Data, accessed on February 13, 2026, [https://fgullo.github.io/files/papers/INS17\_UAHC.pdf](https://fgullo.github.io/files/papers/INS17_UAHC.pdf)  
36. \[2505.01485\] CHORUS: Zero-shot Hierarchical Retrieval and Orchestration for Generating Linear Programming Code \- arXiv, accessed on February 13, 2026, [https://arxiv.org/abs/2505.01485](https://arxiv.org/abs/2505.01485)  
37. CHORUS: Zero-shot Hierarchical Retrieval and Orchestration for Generating Linear Programming CodeThis paper has been accepted for presentation at the 19th Learning and Intelligent Optimization Conference (LION 19). \- arXiv, accessed on February 13, 2026, [https://arxiv.org/html/2505.01485v1](https://arxiv.org/html/2505.01485v1)  
38. Metadata-Driven Retrieval-Augmented Generation for Financial Question Answering \- arXiv, accessed on February 13, 2026, [https://arxiv.org/html/2510.24402v1](https://arxiv.org/html/2510.24402v1)

[image1]: <data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH8AAAAXCAYAAAAiGpAkAAADkklEQVR4Xu2YWahNYRTHl7lkHiLz9GCK8kAeDBeFB4kkmecyPJEQHkSmB5mnEGUoQ1IeZIoyvQgZEqUrEg9IhhTF/299+5xvr7vPueemc/Yt369+3XvWt/c5e+1v7W/YIoFAIBAIBALpUQavw9PwJhwWa81Ne3gBnnXegYtjRyjdReP0seh5aVDMPJvAvfAWfObcBLv5B1U3+sCPsIP7zIv9JNph+agLy+EEL1YPPoXTvdhSeEr05tHfsKPXXiqKnSeLYoD7v7eThf4I1ooOqm4U+6aEzq/GnX8AXjSxG3CPiVkmwx82CNaK3hjLHGdanV/MPJuJ5sUC8Fni4gtM/C+sOjbQ8bBpvDkRVtQ1+MH9nRJvjrHIBhJ4C4+aGJ/UFyZmOQFf2SBYKJpwWxNPu/OLmSef7CtwfewIkSGix3Duz9AInhEdFvY7L8NvUvGmRXCooXfhYFgH9oL7RIsgGnIiOMzxwvNRA/6Eu038MPxuYhYm+8QGRYdCJtzfxNPqfOZYyjx9lokeM9APboZD/YCDnZuL4c5xtgH0hZdEL5Jynrktla80uULlxe008YMuHhVcEg9Ef8cyVfTckSaeVuczx0LyzEVV8ySNne9FpwIWX4bVcJXovLHd2dw/IAE+6ZTwy1p4bRGtnD1gTTE/mgAXM0xgl4kfEn1SoqcmCRYXRy7LNNHvHGTifud3ijcl0gYuL1C77fJhjoXkmYuq5snv4laSnhT97QzsFM49K+AW0SKgUcdWxgj40sm9JAspV+Wus4EEPosuhnyOwXcmZjkPn9sgmCt6U+wq2u/8zqYtCebUr0B7unPyUao8t8GtThYC+zvzYLPzZkYfqkBDJ19ORC9JuMpk5d+HkyRb5VyAjJWKw1wSXGtwC+ZzVTTpfLBgv9ogWCO6GK1t4n7ndzFtpaAUea4Uned9OCVviD7Mh7OybRnYWaziXJQ57TBDWosu/F57sqo511UG5643kh096ovOVXZtcRw+lOwI1VV0C8RFpw9vMKcxi9/5PLfUFDtPjgTc+rGjNzo5srPAMu8DGrjAPXhOdEFBOVzkm3sKgUMMjc0zBbBDdA88Q/Ta+GLGckT0mv1Knyi6GJrn5K6FCxx/Cpst+kq13PlFdOvEHQqfilJSrDy5Q/slWthJVtgR8E3TaNjSmTbt4BjJvgErFG5bRzm5vfzXAi421SLP0Pnp8L/kGQgEAoFAIBAoNX8ABVAND+CysDIAAAAASUVORK5CYII=>

[image2]: <data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAGMAAAAXCAYAAAAfiPFCAAACtklEQVR4Xu2YS6gOYRjHX8f9FnIpNufkLnI7JUQuibCjSDouC0qKpCi55KyUUhIWLuW2IQuXhSKdQiJspNghGxbuCyv8/9/7TOd9n7mcKc4375zmV7+aef4zffP1vDPzvmNMRUVFjbXwIZyhg9AYAGfrYuDMFIfrIIUG+BH200FI9IR34TQdBE6jeA92V1kSzfCBLpKJcDt8KTZ5aX05DU+oGu+UFfCUeMaPg2I/3KeLxt4Be+BukQOu1TtCgqvwFvwjjvaOqB9T4Cc4xKlxm6ONDfoiXnTy0OgNX8Hpqn4Tznf22+BCZ79G1Yz/yz81I2KzKb4ZZ+FBXXR4J4bcDLLF2AEUMQq+l+0eIgcdG5dICM34DlfrokMIzeDLeQ7cCRfDPn5cYy787Owvhddke554Hy43KQ0pshkjRP72VJW5FN2M8fAFfARPwufwjXeEhdNb/peRss+X9zm4AR4Wb8AdkscoshlcU9DfJnveXXQzbhs7aFwSRzb4Cpeo2kBnu5ezHcNtxhiVafgMpHtzyls6i3XiBx0oomZc0kEKbLC+ljRXyTlZXIdb4VF4SGWaZyZj5HeE24yxKtNwNNBo1dmRk+1pqawU3edsElEzLusghWEmfi1pNso5afCO4IxzFzwGN/pxDM6otuliXtxmjFNZZ9Mk8reHeolP1IwrOqgDrSb/47sB/oLLdJCXTaa4ZnQTf5rsb1JvxSKacQFOcPZ5vVxRJ73jeJflecLE4B3RZuyf/CFyTsypF2/fevIUtuiisdf32LRf3zf4BB53julsmo19D/B7Eh9Xr+F674h2+OLmwOJ3ttJyxNhvTyHDAboA9teBA79PcepbaqpmBERfY78aT9JBieBCj2sMLhBLzyx4RxdLBCcXXLd0GQ7ANboYOIvE8zroCgzShcAZLHLKW1Fm/gKi/aI1/AL7vQAAAABJRU5ErkJggg==>

[image3]: <data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAkAAAAYCAYAAAAoG9cuAAAAhUlEQVR4XmNgGAXkAC0ozgZiPyDmQ5bkB+INQHwJimcA8X4g/giVA4MeILaGcZAAOzKnAYgrgLgZivuBmBdZAQg0MBBQxATEi4G4HIi7oRjEBonDgRsQxyILYAMZQByHLggEkUCsDuOA7N4HxGcYIMEAwheBuBWmABkoALEHFAujSg1nAADv1BV8G64IhQAAAABJRU5ErkJggg==>

[image4]: <data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAaCAYAAACO5M0mAAAAwElEQVR4XuXRLw+BURQG8OtvNE1AslE0mg9AVZj2boIgaW81H4Ekqb6AYKPZVCZpVKqK5+x9zvbeOxuiebZfuM85u7vbNea3UyYPuvbITpP2sHRmL7OGoVuGE6cbNJyZlbeLBShBle6Q1mEUJjQHH3Z00CXJCDak2dJUi4wJ3tEmzYk8LT5e7MADciTJshNFdmYAZz0wLbhQBOpS1uAYWkrCChZUgZ4MEib4y3FIH640g9RXi5o8xXiW3xBy0Z/mCSjjMCuilTjqAAAAAElFTkSuQmCC>

[image5]: <data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAE8AAAAXCAYAAABK6RIcAAACUElEQVR4Xu2YX2jOURjHH4aNpjCzi+VCi7ggfy4Vw1a7mAtqLiasXBFp1qKkKBG5WkMuMGLayMUKJUm54woXFEkiCnfKmgv7ft/n+XnPe/a+735v+p0Lzqc+7T3POaed9/md8/zOJhKJRP4x2uBu2OXFI5MwFR6B3+Edry9rpsBe+BB+gJfg+oIRYeiEI3DYfAA7CkaUYTr8AQ/4HRnTA7vt8wI4BH/CRWYIlsMXcK4Tuwx/w01OrCQxeX+RvLWig1f4HRnzHH6CM629UnQdt8wQHBb9nZud2HaLsW8CXGyzyc9H4VfRGhSSU6L1pcra/DkGn5ohmAXXSX4N5Kxo8pY4sRzM6jPRI0oH4Xt42xlTDCaWWzuts3VaRawRXTRfYDQk8+BO8wvcU9gtsk20pix2Yk2iC97rxIrRCC+U8LzjOdgPT+i0irgP38L5ZkiOwWvmYynMUW57foRX3aDo0WXylnnx0OyDL0UfUjl4Ag7CQymt12kVsQOOwvYk0CKapF1JwDgOP3ux0GyFj+Aca/MLl/vSfEOuTilvEuXgpvJLDI8wc/UkCXRZYGESMDjgJqyFrV6fC+fxKpHWizptUjbAG7Daid01s4a7+B38BmucOEsGc8WTmiOpbckT3Wj+Eq133KpbrC8Uq0QXeAaehKfhgGMI3ojWOPemwU3EXPU5sdxuuCe6yKQmcNfx7XsFTssPDcIr0UX68n5V9I6VAUvha9GHtd9k+zqckR+m1ImeaZcGr/2/wb/vWUf5DxLq18A/xORNJHXyIpFIJBIpyTgxy4MCzV8lIAAAAABJRU5ErkJggg==>

[image6]: <data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAFcAAAAXCAYAAAB+kNMAAAAChklEQVR4Xu2XTYiOURTHj+8yvsZn8rWwoUwZhFjZiSzsLCYLETuJMbIlSixkBiUMjfItWUhJ02RixczCV1GKlcV8JIspZf5/59zpPneeZ+ZR8943dX/1653n3PvWfc9zzz13RBKJRCKXergbHoLTzcQYsRO+gr/gJLNaLIH3zJvwYHY4CtPgNnjZvJQd/sss0Xgn/Giehsv9SY5H8FkYjMwU0cUuteeJ8C7cNTSj8syGz+EF2GdezcxQHsL19vcq8x3shhPcJEdKrjLmyR0Pe+AxP1gFNsNPkl3cdtjuPcfkuxkmly/gD7wfxA9bfL8fXGPBjXCLucyfEAnuWK6DZ1mNxc5I9V56UXJZUdzdJ4M488b1n/KDLuO34R7zhX2OxAxY+w+WoU10LV/hedGGUq0GW5TcIo6Krn2DH3wC34p2SQevZt+85zzOiu6y0Itmi9kseoaxnEaDXfgzHBBd6B04NTMjHi6518KBgJnmD9Fbzjg3wPONh/YRFzAaRX/cgiBeSebCLrgXroAdomt44E/KgcdZU0l32HfK4JJ7PRzwYCJ59tJbElTZatEfsNYPiu6Y3iBWaXi+cuc7uHDGuL6FZh58KewbZeQ9uiwuua1B3IdH1zmT6+XlYI4b3Ap/ix7SZLLZL1rOI3FF9CWUdb5+rZCn8EAQY2P7CVeaMRktucdleMXXi9foFosml2+fcDL9ItqwYsLG+iaIMaEfRHcEjYlL7o1wAOyD70VvBvzPjLLKeBFo8ObJCfhYtFPzk7pkx4TV0yraXPmCuQNewjpvTqVhabfD16IVQ1nFfGbpk0WiG5LHVZ7rbN4Q8yT+Ti2C1cTGs0n0iPrvSclNJBKJRCIxnEHjEaDSa7iG7AAAAABJRU5ErkJggg==>

[image7]: <data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACwAAAAUCAYAAAD2rd/BAAACX0lEQVR4Xu2WTUhUURTHT1HYBy203PRFCQa6KNJNbQwjij6oXPSBgUglKkEU0UZ00zaLvhYtiyJqERVtCkVbRlEtAiko2lgRVFgoFEH1/3PPdc6cec/maRJCP/gx8855M+/OveeeOyL/mRDz4Q64zMUnlXIfKJIKeBmugM/hhvy0zIFzXayAhXAvPAirXS6JZtjpg0WyScKAyTV42uTIAnjfxUZZCR/AW3Ar3A1fwXuwVPXUwQFY4hMZmAHXwocSZtpzwAciU2bAR9RBuNrlZsEX8I5qmQ5fwy0unpWlsA3eluSNN81eNMFfar1NGLold0+ViTdIWJGJwJXhDJOzsM/kClgMhyQsOU1jv+QGvMvEOeOnzLVlnuSXCXd7UkndhBf0/XH40uQKuChhEKwfmoad4e0mzlJpNdeRSnhDQpviKpxU78JLaoR7hV1iH3wKt5lcHqyLt/A7nKmm0SO5AbNvEtbvN7g+3mQ4D8tgL/wsuY1UA3+qizRGZkvY8LE0EuGDOYBnPuFg2fBHPVHjBlgi4fN89fDU4sO/wsMmvgf+UFkymWBD5gPZwsbijIT7eALZUyh+frmJWdZI4Sa9KmGT0nHxDr7xQQOXaQSe8AnlA9zog0qHhO+P8HgdlnB6UpbEKpMvCm4YzsI61VILP8KjLm5hCzrkgwrr/oq5Zq9m7fI/B2VnSFudVKbcgEkX/KLyfYuEuu6Hm819SZyTUONJfII7zTU373sJLYy2m1xm+H+U8qTjTMTW9SfYDR75oMK25mHr5Gal/4zrsNEH/zJj9uescCYfyzj6agaO/QZR6n4YmyAY/gAAAABJRU5ErkJggg==>

[image8]: <data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAFEAAAAUCAYAAAD1GtHpAAAEBklEQVR4Xu2YW6hVVRSGR2U3y7RM0Mo7kdJVfYisHoooxUi7CD6YEIammARWIEbuSkrtRhdICO+KkmQZmKmVEoKEL1lGRSW+aKao3SANzP7vzDHdc8+z1rl4xHZ2PvhkrzmXe68z1phjjrXM2jntOEveLnvmE/83uuUDLeRc+Zq8U34kn62d/u9ymRwjH5bXZHNFjJNP54Mt5F75tX8eJI/IS92cS+RV8oZ8op64Vm6S78u75QPyO/mx7Orm3CK/keflEy3kHNnXP/N7+y0sb8x5Xf4iv88n6on2ILaBqe5uOSSbo2btkOvclDPlDxYC3lbOkJ/I+/KJjNlWh0F8UB5z78jmIs9b9Zzrk/FR8rPkuC1QU7kZ3LSyTAQ2nroK4uXykNzoljHWqkHkc4Rl/0pynHKhhYBEzpcXJ8cp4+WtFs5/UnZ2i6hYeRApNzfJLvlEAhnPxhm50u2fjKUUbXA9LHxPA29ZCMzNbhlpJo5OxqmFk5LjCBf1jvzKwvKcK1+Sa+QyNzLcqt+Ne5K5IirWOIjcoAVyoYWOYr180RpnNDfrc/mC3Czf9c+4rXracZ6TG+RKl5W6VD4jv+UEIkkN/Eue7Zax1qp/5EAf4/8ftuISwAZAO0J2H5QDfJxSEL+nj4+1loo1DiLBY1VELrJwDiUitl408n9b2AiBvpTrZ3VgJx+PXCHnWEiueM0kXeQ9/unnE9uTiSK6yz8tZBWymQA/wv/v5ccpIy1kwK/ysWT8fnnULVvazVGx2iByfVzHI8kYvCx/d7lmygXn0YvCYD9m+WMO/TExesJCx4BpohGLhrXOl6R3sAiWBeeNcCPUH8b5oSJutDB/dTK2SG5xT5SK1QYx/s5DyRiw5GIG9faxLyz8PdRrShTHBDgmRhF0JKvcCC0Z39sAy3nX8anGEADuJG1FEXvlsHzQmW5hPkLd+s1CxiDFOWZFayA4tFWRDhayZFoyBtTkn924wVGLJ8v5Fq6PZd8UZN4fFup+WvufshC7BiZYiOhtbgp/IBfHblkGfd2j+aBDQV6eHN9l4bdYfsjzMhtQa5klf8zG6BA+tWpG0cDvk1Nc4CbulEPldRYeH8nIpuBcrjnu4JGtcl48aA9i07QoiEBqsgHgTAstwmoLLUBzTyIEgp24iANW+/RBb/aTXOyWBb8p3rawPCkLtCpIvaNTICE+kG/IL+VEq21xOIe3RLFOIhvmEpfA5/AUR93MoZ5uygdpR5BsZPMoazxz7rHi/gr4vhxqTNkbmpNBRwsbXdrkRwgIm8MFfsw5vCugvuLjPp5C9mIRJ/rqr5AVVvsUU68QwBn5oIUmHV/NJ04l9HtkY3M73b8NbQlvo9608LaI96U8fXzotjqz/gHOiu7Kx4wfkwAAAABJRU5ErkJggg==>

[image9]: <data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEoAAAAUCAYAAAAqVKv2AAADxklEQVR4Xu2YeaiOWRzHv4xtZmRtmAwiW0iosSZukT27ZElXY2lsqUkNZYk/zJS9SBFlK7sQsl5TZLJNjWUyhTJhhBlLZAnfr98595733Of1upZLuZ/61Pv8znme57y/55zfed4XKKKIT51v4sDnwLd0EB1Fm0ZtSQyls+LgW/A1rUNb0NJR24eiBK1Jmzv13V9LY+dBupP2ov3oeXqEVs3rmkJb+hf9Mm54C0bQK/QFbPCFQRY9Drun7JPSmkBRot4gUePpNWfLqK0kPQVLVkwxehEZLl5AuqBwEyVUZp47K0Vtuai+aGAaoExiGqyPZo/0aNYdDY7fB1ko/EQtp2eciVSj/9FDcUNEf9jgRzs9W+mi4DjkC1oxOs5YKEkHpE9UGdqGVo8bIlQmVKSFxtDKmY7LdJ7TUw5518BS2KDa5zYn42dUttNzDrZsQ3RxuQE226bAdk89tQX0Aq3gTCJdoibSfXQMXUL3In/iVTZUJubS7bB6O5nec1bO65pLbdj9ejpL0V/oaprjO/1DnyLzVrwFdrHvnUL16RHt7Ds5tN6llqWSpHU/LGj/F/lnZkhSoobTm7AZ5VkLq5vFneJvOt19/go2vt6w2SGT+IE+g91PzofNSCX5rDr4TP7pTkiHnsIDWNH2s0Vo2ep8XSekq1PJ1+4pPVoGGtRAZxJJidLOuzE4FrqH+rVzate9T38K+tyhM4LjJNbTW7AZL8u6uN4AquiDEqAb7XAN6ZgN6zcgimt3ULxeFPcooXfphCCmjeMxLGFh/QpJStRDuiY4FkqO+mlZS6GlfRj2K0Eb0/+0vmtLx3W6G7bMpM7XJEhBS+9qHAxoALvZwrjBodeJ7nHQ0Rr2RfRkPHvoNtiyld2CNk8W8idK5+wKjsVIWL+wUC+mQ+gyWJ1p6OLpULuuoYfjl/AfsPNF7vj8zTr5QEATegN5az6J/XRSHHSoPimRHtWXJ7B64wvn2KDd0xE2plpBTPXuNlJ/T25G/tWg8WjWNqONkPn354+w2RrW6GN0DuxcbRqvKErUGyZKTIXVkplOJU+D+A2Z37hVE1IuFqCdUkshJMfFVzrj3VaJuwTbPFTA+zpFe1hiVAZOwnanuM79DEtyaA6shMiYFbBlHaLdWC+e62iNqO1VYc5y9qB1w8bXoFlxGlZvYvQlkuLxu09B0AahXbZ83AD7l0M783ew+6qvPm+CzTQZo3e58JXDo3ipOPiu6H0mOw5+BMbRA3GQDIbNTvlR0dM9gfRv2oWFZsYq2NJWHdSS/ZX+DtuBZYF5CWdL4dUa8wdFAAAAAElFTkSuQmCC>

[image10]: <data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC8AAAAXCAYAAACbDhZsAAABvElEQVR4Xu2UyyvFQRTHj/ejCDuSnY1QSohSioVsEKWkCFnYIBuvZKVsCIWUpFCK7IiSP8KSUsrCSiEp4nvuzFzzO/fx+y1Qaj716d45c5rfPM4MkcPhcDj+kHQZiEKaDPwE2bBUxApgPQWbFNMMF2GC7NCMwwEZjEEFzND/U2CVtjicoeGkY3gON+GMdh4OwydYGM6OTzfcgIkiPk1q8kEYhKvwDvbBHf3LXsCl71SiftgEp+A7qR1kDTzIiNX2owtuwSTtHBz1ZMSGF70Lk+ErvCS18wae66PVplpSySfw0O4AefADdoi4H53wSjsm+uKRCSthDfyEZd7u0K7zmB54dc+kysSmB77BHBGPB9f8MqmjZ3kn7d0LwgR8oMj7cwMXRCy0+7zSEhE/hUekBmkXfdHgvDVS9WngktyDqVbMjzN4YLX5JMxplMNqsu7hJLw3DQ1/jHe9F7ZQ5KlIeOLrpC6tpAHuU7AF8Cm9wCErNqu91W1+YMJjca2vmIYFv0CcuE3+b3QrbJNBizoK9lRynXMJF1kxPj32mlQlNFp9oYsp68uQT7H7fgP+Vq4MarK0Hv715B0Oxz/gC1wxQ6un6zn0AAAAAElFTkSuQmCC>

[image11]: <data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEoAAAAUCAYAAAAqVKv2AAACX0lEQVR4Xu2YTYhNYRjHH0XIN/loIhHNwtcGC0qjGUNoko8sZjNqTBYWFhZSJNa+illYmMVoSmExlCKKms2IjQgLG9JYoZSGBf//PO9p3vvvnplz5945Zri/+nWb53/OPfd97nnf8841q1JlrDNfC/8Di+BBeBiuk6wYzfCs1GbCKXBSeJ1aGA8wDU43z/g6LlgVfATvwia4F76GT+DCwUML2ATf2GAjOHh6D36Fv2EvPBLymPvmeT/skmzMUm1UBo7CT8GNknHqPDdvljIBvoN7NAhcNW/ECg0Ci2EPnKNBBhbA5VocTbi+cDDbg8U4ZX4M7x6awLuOA03juPl59RoEbsLVWszINtiqxdGiBn6BjzUQ9pkPuC2YcAdejv5WDpifd0jqnNJUHwCl0Gg5NqrdfCBbNBCSO6olmPDKfNqmwWnM885EtdnmXwydHNVLZYfl2KiP8JcN/4Fvmw94fZBwffph/s2mwQcAz+uIatfg5mA55NaoZeaDeKmBMA9+N1+0JwYJpy3P5/sMBZuZTO2t8EqUZYVP4xbxPLxepE43WAVhAzjQbg2Ec+bH7Zf63FBfKXXlLXxvvn1gw2YUxplYYr54x56EF4rUadpTdsRw6n3QYkSt+V7okgYBbid2alF4AH+aD2q4Y0sht6lHeCHeFQ0agDWwD57WIOIhPKZFgWsSr9GpQZlUG5WRXBtFONe/mT/CKS9+Cz619B13wkXz3fdQnICfzdfESpJ7owgX5rrgLsu+GO6GL8y3CmkshWu1WAH+SqPK4YYVbkLzgj/j8P+9ccMs+Mx8x/3P8Qd03oDzrHi7jAAAAABJRU5ErkJggg==>

[image12]: <data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACUAAAAUCAYAAAAKuPQLAAACB0lEQVR4Xu2UPUhWYRTHT1T0saRGFG1C5hDhYIQ0REOYX4hY0aCEgzpEgWuBIgmumqBL4NQi+AE6NIiSipPUopm0JqKTCkL0Qfn/c87z+rzH69erkEM/+MF7z7n3vec+z3mOyH/+HZd84ADs69kr8AlsgAUul0QNfO2D++QEHITXfYLcMMfgCKyE1XAeTsDLW7emcQcuwHM+cQDy4ZQPkmNX1HO4ZN52udPwo2hhHi79V1jlExnQHl+wH/7CB2YSzaL3cFVogKs5HV0fhvPhx1W4Cse3cok8FC2q0QywQd9E1zEnYba75gHakx7Rl931CUdYqToz8Fl062NOmX2iq/hS9BS/hR3wC8wyE1mEv+AZn3AMiBZ1yyTsp++wONxkcJRQbi0L+gNro/yKbF/xFLmiL5r1CcdFuCHa0GEVCLeez/N/YkpMfihPMQ1wO3/Dx+Y2+DL+6bBPONpE73vk4jkWz3PxAItfhy+iGA/VD9Hi4n5Lg9v3zQcjOD/WYKdPGBwhZT5oFIkWzdkXeA+HRLeelka5FPWiD973CXATLsMWn4gYhU0+aLCfWHTgLPwJn8IK81mUT3EsiyKvRPe+1WSh/XBS9p7UPOLdPmjwxHa52AeL95q7nno27T2zHF6Lk7vAr/0k2h8eNnJSfKcBuuPcyoR3kj5QM4WD/Mi4AGfkcF9aCOc2ASLRan20/ZR5AAAAAElFTkSuQmCC>

[image13]: <data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADsAAAAUCAYAAAAz30VQAAACCElEQVR4Xu2WP0hVURzHf5aDRQ4lhQWaBNUqUlNhQw6SViri4CAVaSoNDWEN4eQUQaFQLS2FkASCIiIFTuomtohGLrqYiuSfoak/3y/nPDnv632PSLi8l+8DH3j3+zv3vnvuPeeeY5Yjx99yFB7UMBs4ARthG7wA85LLuzgLRy2LOnveO+ath9fhtLdsp2Uyx+AcvKiFTGbfdJbD9Zv3stQ4ND/BWf9beQlfa5ipNMHf8KY3ig5zbRokL4Ib5t5uxlMM1+GkFoRKc53tlrwTzktGDpj7OtMwOxkcx06fuU5UaUFoN9fuseS9cFiyfDgAJ7x8QLfgG/gUfoWnEo3jZAn+hIe1IHBOsrO1ko/AZ5JxWtTBh16edzeos7OPguNYOG3uRr5oQeCDWIUrsFBq/Arfk6waFsBB78egdgj+gK1BFsUDuAXLtfCv8KPCznKZSQf/mO04P5UZi845P797+XYT3IC/YEmQRdEMP8MzWtgLHMZcblJRCpfhW4tedt7BFxqa23XxAdGKIP8Ax/3vmiCPhdvmbuiaFsxtARfhc0u9XeTc4zZR6YJr3sS5fNvb8D68BJ/4PDb2VWcJb3gT9njvmFs6pszNnXRwKC5oaG64vvKG8Os9BPvhEanFBhf/K14uL+cs9dsMOW5uU6LrJq/H86OuwY1MVJ4VtMD3Gv7PcNhf1TBT+ANDcmuFzjr46gAAAABJRU5ErkJggg==>

[image14]: <data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAZCAYAAADnstS2AAAAzElEQVR4XmNgGLqAEYiDoDgDiK1RpVEBOxDXQvF/II5HlcYEqlAMUgyi8YIkKH6FLoENzIfi9Uhi+lDMgyQGBiQpvg3FxUDMDcRTgHgxFC9DUscgzgDxGAi7AXE3EPMB8TkonolQCglfmOKtQCwAFZeBYhYoHwz6gPgiFM9igDhHDVkBMjgBxE1QDAIbGFCt9oYxOID4JxC7QzEIrATiDgZEaORCxUlTDOJ8ZoCEJSw8AxkgobAIiuHhDEpxojAOEgCFCCgUUEJiFMAAALl/Krhvrcv7AAAAAElFTkSuQmCC>

[image15]: <data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC4AAAAXCAYAAAB0zH1SAAABjElEQVR4Xu2VPSiFYRTHj48YGHwMEonBorCYlJJBxGQjHyllEYtFPsqAyOgjImWzSLJaFEXZZFA22UwyIf7HOa/7dLq9Xrr39d56f/Wr+/7Pc7vnPvc8zyWKiUkJ3XAE9tpClMmHs/AZ7ppauimCW/AC3qoLsNpZ40shfIWDtpBmDmGTvm5U7+AlzPIW+ZGxjXfAD1hlC2mklOQzD0w+rfmAyb8ogK0qz/gyvHfqYZALT+G8yTtJGp8xOQ3BKziu8jd+oJ8PZh4s/oW8IX9hjqTxejfk6+4F1jhZA8nCfidLRhvc9HEDrqtrsEfeFhgeHfYJ7rsFHo9HuOOGoIuk8QqTh0k2PFb3SMbom3aSBvvcEKySnOT/hH+tJZXJgSVecZik8XIvUK7hNsnP1GJqLnzz8HkIKp+lIPCf34TJmuGU91BL0jgfHIYbYd9J7u8xklsmTEbhDVykxI6vwDMyZ4Tn+0SLk+oRPCfZ9UCXfoqohG8km5nMusRSgUfC23WPMvMcSTK28ZiYmAjxCYvTVuuQmcw2AAAAAElFTkSuQmCC>

[image16]: <data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC4AAAAXCAYAAAB0zH1SAAABmklEQVR4Xu2WTShEURTHjwjFRpSvUsSChbJSiiRJsbJCPlIWFmJjy1akWPgokZVkgwULWdsoW0msSLGVjY/4nzn3TWdOem+UeWbq/epXM//7bnPm3nNvjygi4k/oheNwwA6kM3lwFr7AbTOWavJJfvsEHijr9UN+FMJ3OGIHUsw03HGfc5wP8ImkpkAytvBu+AWr7ECKOSNZMF3kLkktDSqLUwDbndzjC/BOjYdFOWwy2RW8NlmMUXgBp5z7JNsTdDBzYdEv5AVJhixYAVect7A54QmS6+4VVquskWRrhlT2Ex1ww8d1uOZchX0yLZAykoJPnTw/ob+5PR7hlg5BD0nhlSb/L/bgPSzxgi6SAge9wLEEb0wWFlxctsmGSeqc8YIxF/CB0FzCTVgMW82Yhm8ePg/JymfJj1r4AQ9NzvO4Tm6ZGHUu4IPDcCHsJ8n9PUlyy4RFDXyDcyZfJqmzU4fc38dwkWQr2CN4TrLqfMLDpJ+kTedJ/gD7DCf0Qx7cEt6qe5Sa72HC7yttsMVpez5OxhYeERGRRnwDGTJWcnl2t5EAAAAASUVORK5CYII=>

[image17]: <data:image/png;base64,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>

[image18]: <data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACIAAAAYCAYAAACfpi8JAAACKklEQVR4Xu2WT0hUURSHT/8sSlLJqCgXCYFkCGaLCKtFBFHhxkDdRK7MjSi1ibI2gYs2VosWga1KC0VwFUFgRBCIgoSLlgZBEeUiAhEyfz/PeTP3Hd+MMzrgLPrgW7xz7szcOfecOyPyn8JyDm7ywQwchLU+WAha4WMfzMI2OA73uvi62fCN1JtfYIXLrcZ5OOyDa+W1ed0ncmQUtplrpgZ+N7e4XK40wG/mCnbBk6LnSMlxeCy1QrkD35uZ2G7y/fihW+Pp5c/6Zx4IE1z4HL4RLRt9CDtFz/N+eqk8C0yCI/rBvAl7Rd/P89Xk+Ke4Bi/D2/CXedhyJ+ACLLPncdF1NAluus8k7fBzOp3indkVBk/BEtEGfGlGXBQtYaM9z8IWM4kX8KfJ0T4rehSeAfNJGCyajRD2yW/RkQzH8q7oRvbY8yfRo6RJHIUzJl83D0/HViiD5iOfYIfzhRxPGjENR4JnVot9EDZwRAdsCp4PiV56D4JYxITZ7RO3RDdSaZILoo27P1oE7snK4yOl8C+8EcQ2w4+SXJE5M9z4MhzdSThm9sNXsDpcBK6IrqMe9sRT0c1SXglXYysUHnN0jxwJE2zUP6Kl5begVeGCAJb7h1nucmSnaAXDKnouSXp8Y5yRdH/kQo/JSy9fOBRs+GYzxhRcFD0O3ow0G1HV3opOST7wAhvywYii2UidyR85ls7/SGWCjZzPf4t9okPBPio4u30gCzvM4mYJoNJ9ITKGrzEAAAAASUVORK5CYII=>

[image19]: <data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACcAAAAUCAYAAAAOTSQ2AAACPUlEQVR4Xu2WTUhWQRSGDxlZSVBWBkllIeYmwmwTIVrQjxAuCiQXGkQI2iJIlCKwjYvQQkyj2kTQIogSXEREtYiM/EFaJrTrZ+EiCNyYZPW+zBk8nL57zVT4Fj7w+Ok7c+fOnTtzPkWWyS42wBwfzoONcIUPLWmDp11YAp/K3NdXw1q40+T79bMC3jW57IN98L06BdttB2Uv/Ax3qZZ8OC6zN8nESfgWdsBm+AbeUV+afvfM79k7uTptqIRVKif3LHYwPIFjPlRuq0lUwQ9wvcnWwEn1qsk38QcnxqflJiTcD/QnvKlZhH2mYZvLCdu+SzgMSTyH13wI+tWDvmE3XGn+LlZ/w9MmJ+c0L3I5OS9hVdLg5F/D1S6vV+08MhI7chLbXdsLOOSyCFd5wIcOnmKO+xXehw3y90Qzsg5egCMqX9912AU79ZOvujVe4OCN2SeNAvhIwticJH0nYYKpk6yBt+CMOgx71R4J+4WDHYgXOLhvG32YwFp4BA5KGPOomgpvHJ/okGvjKv6Q5Cdk+WnyobIDnvIh2CPhXsfVVLhKX1T/LfBKwutO4gHs9qHCenbFh+CwhJKVpybCr5sJeEP1fJNQKMlmWKZGLknYd5lgbfwEc03GN8Ct809bIasndwz+gqWqZYuEvfERnpWw+cvVyAltt8SCzvrH4st9ya3TAkfhxdmu6fA/BJ7YJLiaj+FlWOjaCMsEV3eryVapLOqEfXhKeZ+kg7VknIEPffifbPPBYsAiy1O4UPr/ALSMg7AcDZzgAAAAAElFTkSuQmCC>

[image20]: <data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEEAAAAYCAYAAACldpB6AAACVElEQVR4Xu2YS6hNYRTHl2fermuk1KUIJaLkipIy88hAGDCQkoGBAaFMdEleiZFHIaWUkbohkZMJIykJGTCQDJVQUvz/Z62jtdfde5/tqns+2b/61fnW+vZp73X2Xt+3j0jNf81x2IjBFpPNJTGROKvNETFRwHJ4NwbJaPjQnBNyqbPFPBoTBRyCB2KQ1EUAl+FJ0zMBroEX4bmQS437sDcGwQx4xNwPX8OlfgJZBD/AiWaLqaJfzIv/DM+7XIrMhy/gOBcbA1/CSSZzn+BIN6fJVdEKlfFR0i8CuQn3ujEfk2tuvBLeduMmw+AXuDYmAikUgY8mz3M3XCB67pHD8LobH4O73LgP7oMbXEymwZ9wrg/m0OkirIfvYD+8BN/DU36CsRU+c+OF8CzcbvIaLoh+32+4Zv4QXR3K6FQRuk1e/HAX512Qd85seN8ku2fg57EmGXAcK/c2BnNgEVjBKmw22WeqOE8Py6XVrO+JPgZs0hszM7KwmfPOnh0TZdRFEH02eIHt4BzuFaow01xcUS5dRawwb8CDon1gWWZGlumiReiJiTJmiR7k9wd5sAhsSEPNFXPAul7AKvhdqr9HNGGz+Sr6i5TRiSKw+TVM38y64B439uyEb2KwCk/hphgULVADPhHdMXKn9djk+jsU7DCfwzvwgegSWPR+cwLeisEqnIZnYjAxRom+LnPdL7vVH8F1MViF8aJ7bvYH+q/CXSAfBb+f+CPYgXkbDepW6jCtjdArKV852lIXweCfErTdy1Rq8KWJbouJv6Fs85IiU8yawfALNxd5aFlxR6EAAAAASUVORK5CYII=>